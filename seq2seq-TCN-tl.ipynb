{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-TCN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "W1_uV8WoFXQu",
        "colab_type": "code",
        "outputId": "54596203-8db0-4d5d-f7c9-9f293b1cc41b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/My\\ Drive/nn_output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xscbWz8wGH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "OUTPUTDIR='/content/drive/My Drive/nn_output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrCPRQxBFJqP",
        "colab_type": "code",
        "outputId": "c2112089-2d73-4dbe-b7cf-54f362520099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-TCN\n",
        "\n",
        "from keras.layers import (Bidirectional, Dense, Embedding, Input, Lambda, InputLayer, Reshape\n",
        "                          , LSTM, RepeatVector, TimeDistributed, Flatten, Layer, GRU)\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from tcn import TCN\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import numpy as np\n",
        "from nltk.corpus import reuters\n",
        "from itertools import chain\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os.path\n",
        "import glob\n",
        "\n",
        "USE_GLOVE = True\n",
        "MAX_SEQUENCE_LEN = 100\n",
        "MAX_NUM_WORDS = 10000"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-TCN in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (2.2.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.11.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (3.13)\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5lh2ZJRsChy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(fillvalue=fillvalue, *args)\n",
        "\n",
        "class ReutersGenerator():\n",
        "    def __init__(self, max_seq_length=250, num_words=5000):\n",
        "        self.tok = Tokenizer(num_words=num_words)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_words = num_words\n",
        "    \n",
        "    def _gen_sents(self, fids):\n",
        "        return (' '.join(sent) for fid in fids for sent in reuters.sents(fid))\n",
        "    \n",
        "    def fit(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        self.tok.fit_on_texts(self._gen_sents(fids))\n",
        "        return self\n",
        "\n",
        "    def count(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        return sum(1 for _ in self._gen_sents(fids))\n",
        "    \n",
        "    def inverse_transform(self, X):\n",
        "        return self.tok.sequences_to_texts(X)\n",
        "    \n",
        "    def generate_pairs(self, fid_startswith='train', bs=32, \n",
        "                         max_seq_len=250, forever=True, shuffle=True):\n",
        "        fids_in = np.array([fid for fid in reuters.fileids() if fid.startswith(fid_startswith)])\n",
        "        index = np.arange(fids_in.shape[0])\n",
        "        while True:\n",
        "            np.random.shuffle(index)\n",
        "            fids = fids_in[index]\n",
        "            sents = self._gen_sents(fids)\n",
        "            for batch in grouper(sents, bs):\n",
        "                seqs = self.tok.texts_to_sequences_generator(text for text in batch if text)\n",
        "                X = pad_sequences(list(seqs), self.max_seq_length)\n",
        "                yield X, to_categorical(X, self.num_words)\n",
        "            if not forever:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSZgXz1tHKqk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuters_gen = ReutersGenerator(\n",
        "    num_words=MAX_NUM_WORDS, max_seq_length=MAX_SEQUENCE_LEN).fit()\n",
        "n_train_full = reuters_gen.count('train')\n",
        "#n_test = reuters_gen.count('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6v0kPz67sPl",
        "colab_type": "code",
        "outputId": "5e7bccdd-7283-4024-a493-1285f5aacea1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "  !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "  !unzip \"glove.6B.zip\"\n",
        "\n",
        "# get glove coeff matrix\n",
        "embeddings_index = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# prepare pre-learned embedding matrix\n",
        "embdedding_dim = 100\n",
        "word_index = reuters_gen.tok.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-17 07:42:30--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-01-17 07:42:30--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  94.6MB/s    in 9.3s    \n",
            "\n",
            "2019-01-17 07:42:39 (88.5 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "der9TGExhmUz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def channel_normalization(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\" Normalize a layer to the maximum activation\n",
        "    This keeps a layers values between zero and one.\n",
        "    It helps with relu's unbounded activation\n",
        "    Args:\n",
        "        x: The layer to normalize\n",
        "    Returns:\n",
        "        A maximal normalized layer\n",
        "    \"\"\"\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "\n",
        "def wave_net_activation(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\"This method defines the activation used for WaveNet\n",
        "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
        "    Args:\n",
        "        x: The layer we want to apply the activation to\n",
        "    Returns:\n",
        "        A new layer with the wavenet activation applied\n",
        "    \"\"\"\n",
        "    tanh_out = Activation('tanh')(x)\n",
        "    sigm_out = Activation('sigmoid')(x)\n",
        "    return keras.layers.multiply([tanh_out, sigm_out])\n",
        "\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    # type: (Layer, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
        "    \"\"\"Defines the residual block for the WaveNet TCN\n",
        "    Args:\n",
        "        x: The previous layer in the model\n",
        "        s: The stack index i.e. which stack in the overall TCN\n",
        "        i: The dilation power of 2 we are using for this residual block\n",
        "        activation: The name of the type of activation to use\n",
        "        nb_filters: The number of convolutional filters to use in this block\n",
        "        kernel_size: The size of the convolutional kernel\n",
        "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "        name: Name of the model. Useful when having multiple TCN.\n",
        "    Returns:\n",
        "        A tuple where the first element is the residual model layer, and the second\n",
        "        is the skip connection.\n",
        "    \"\"\"\n",
        "\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding=padding,\n",
        "                  name=name + '_d_%s_conv_%d_tanh_s%d' % (padding, i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    elif activation == 'wavenet':\n",
        "        x = wave_net_activation(conv)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x\n",
        "\n",
        "\n",
        "def process_dilations(dilations):\n",
        "    def is_power_of_two(num):\n",
        "        return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "    if all([is_power_of_two(i) for i in dilations]):\n",
        "        return dilations\n",
        "\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
        "        return new_dilations\n",
        "\n",
        "\n",
        "class TCN:\n",
        "    \"\"\"Creates a TCN layer.\n",
        "        Input shape:\n",
        "            A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        Args:\n",
        "            nb_filters: The number of filters to use in the convolutional layers.\n",
        "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "            nb_stacks : The number of stacks of residual blocks to use.\n",
        "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
        "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
        "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "            name: Name of the model. Useful when having multiple TCN.\n",
        "        Returns:\n",
        "            A TCN layer.\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=2,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=[1, 2, 4, 8, 16, 32],\n",
        "                 activation='norm_relu',\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=True,\n",
        "                 name='tcn'):\n",
        "        self.name = name\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.padding = padding\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
        "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
        "            raise Exception()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
        "        skip_connections = []\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i in self.dilations:\n",
        "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
        "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
        "                skip_connections.append(skip_out)\n",
        "        if self.use_skip_connections:\n",
        "            x = keras.layers.add(skip_connections)\n",
        "        x = Activation('relu', name=self.name)(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_slice_index = -1\n",
        "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXHnYTNY7zsD",
        "colab_type": "code",
        "outputId": "74cf41a1-4c48-4cdf-f3f7-66d344ed1a4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "kernel_size = 3\n",
        "n_dilations = 8\n",
        "n_hidden = 256\n",
        "embedding_size = 100\n",
        "dropout=0.4\n",
        "encoder_type = 'GRU'\n",
        "\n",
        "TRAIN_MODEL = True\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 1\n",
        "\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LEN,))\n",
        "encoder = None\n",
        "if not USE_GLOVE:\n",
        "  encoder = Embedding(MAX_NUM_WORDS, embedding_size)(input_layer)\n",
        "else:\n",
        "  encoder = Embedding(num_words, embdedding_dim, \n",
        "                      input_length=MAX_SEQUENCE_LEN, \n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      trainable=True)(input_layer)\n",
        "encoders = dict(\n",
        "  TCN = TCN(name='tempconv1', return_sequences=True,\n",
        "              kernel_size=kernel_size,\n",
        "              dilations=[2**n for n in range(n_dilations)],\n",
        "              nb_filters=n_hidden,\n",
        "              nb_stacks=1,\n",
        "              dropout_rate=dropout),\n",
        "  LSTM = LSTM(n_hidden, name='tempconv1', return_sequences=True),\n",
        "  GRU = GRU(n_hidden, name='tempconv1', return_sequences=True)\n",
        ")\n",
        "encoder = encoders[encoder_type](encoder)\n",
        "#encoder = TCN(name='tempconv2', return_sequences=True,\n",
        "#              kernel_size=kernel_size,\n",
        "#              dilations=[2**n for n in range(n_dilations)],\n",
        "#              nb_filters=n_hidden,\n",
        "#              nb_stacks=1,\n",
        "#              dropout_rate=dropout)(encoder)\n",
        "output_layer = TimeDistributed(Dense(MAX_NUM_WORDS, activation='softmax'))(encoder)\n",
        "model = Model(input_layer, output_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "#print(model.summary())\n",
        "\n",
        "basename = 'seq2seq-%s-model-tl' % encoder_type\n",
        "\n",
        "outfname = os.path.join(\n",
        "    OUTPUTDIR,\n",
        "    basename + '-ep{epoch:02d}.hdf5')\n",
        "cp = ModelCheckpoint(\n",
        "    outfname,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False)\n",
        "\n",
        "if TRAIN_MODEL:\n",
        "  history = model.fit_generator(reuters_gen.generate_pairs('train', bs=BATCH_SIZE),\n",
        "      #validation_data=reuters_gen.generate_pairs('test', bs=BATCH_SIZE),\n",
        "      steps_per_epoch=n_train_full//BATCH_SIZE,\n",
        "      #validation_steps=n_test//BATCH_SIZE,\n",
        "      epochs=EPOCHS, shuffle=True, callbacks=[cp])\n",
        "else:\n",
        "  list_of_files = glob.glob(os.path.join(OUTPUTDIR, basename + '*.hdf5'))\n",
        "  list_of_files = sorted(list_of_files, key=os.path.getctime)\n",
        "  assert(len(list_of_files) > 0)\n",
        "  model = load_model(list_of_files[-1])\n",
        "  print('Loaded model from \\'%s\\'' % list_of_files[-1])\n",
        "  \n",
        "X_test, X_test_hat = next(reuters_gen.generate_pairs('test'))\n",
        "print(repr(reuters_gen.inverse_transform(np.argmax(model.predict(X_test[:10], verbose=1), axis=2))))\n",
        "print(repr(reuters_gen.inverse_transform(X_test[:10])))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1258/1258 [==============================] - 397s 316ms/step - loss: 0.3639 - acc: 0.9627\n",
            "10/10 [==============================] - 5s 467ms/step\n",
            "['fundamental brokers buys part of brokers fundamental brokers institutional associates a leading inter dealer broker in u s government securities said it has agreed to acquire certain assets of government brokers inc for undisclosed terms', 'acquisition documents have been signed and are being held in escrow pending the receipt of certain it said', 'in conjunction with the sale of assets has treasury bills notes and bonds fundamental said', 'industry sources told reuters yesterday that fundamental was close to acquiring the government securities brokerage division of a major broker of corporate bonds', 'fundamental said it intends to use the facilities formerly used by to provide a new block service in the most active treasury issues', 'by the execution of wholesale trades from the heavy volume of smaller lots large scale transactions will be the company said', 'the new system is expected to substantially enhance the liquidity and of markets fundamental said', 'the wholesale service will begin on or around april 20', \"canadian bank rate rises in week canada ' s key bank rate rose to 7 20 pct from 7 15 pct the week before bank of canada said\", 'bank rate is set 1 4 percentage point above the average yield on the weekly issue of 91 day treasury bills']\n",
            "['fundamental brokers buys part of brokers fundamental brokers institutional associates a leading inter dealer broker in u s government securities said it has agreed to acquire certain assets of government brokers inc for undisclosed terms', 'acquisition documents have been signed and are being held in escrow pending the receipt of certain it said', 'in conjunction with the sale of assets has treasury bills notes and bonds fundamental said', 'industry sources told reuters yesterday that fundamental was close to acquiring the government securities brokerage division of a major broker of corporate bonds', 'fundamental said it intends to use the facilities formerly used by to provide a new block service in the most active treasury issues', 'by the execution of wholesale trades from the heavy volume of smaller lots large scale transactions will be the company said', 'the new system is expected to substantially enhance the liquidity and of markets fundamental said', 'the wholesale service will begin on or around april 20', \"canadian bank rate rises in week canada ' s key bank rate rose to 7 20 pct from 7 15 pct the week before bank of canada said\", 'bank rate is set 1 4 percentage point above the average yield on the weekly issue of 91 day treasury bills']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L7AgCeZ3E_8D",
        "colab_type": "code",
        "outputId": "7f4eeac5-3b51-4acf-8ef0-83d3184adec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1258/1258 [==============================] - 282s 224ms/step - loss: 0.3309 - acc: 0.9592\n",
            "Epoch 2/10\n",
            "1258/1258 [==============================] - 277s 220ms/step - loss: 0.0610 - acc: 0.9933\n",
            "Epoch 3/10\n",
            "1258/1258 [==============================] - 275s 219ms/step - loss: 0.0412 - acc: 0.9962\n",
            "Epoch 4/10\n",
            "1258/1258 [==============================] - 275s 218ms/step - loss: 0.0375 - acc: 0.9969\n",
            "Epoch 5/10\n",
            "1258/1258 [==============================] - 275s 219ms/step - loss: 0.0357 - acc: 0.9972\n",
            "Epoch 6/10\n",
            "1258/1258 [==============================] - 275s 219ms/step - loss: 0.0348 - acc: 0.9974\n",
            "Epoch 7/10\n",
            "1258/1258 [==============================] - 276s 219ms/step - loss: 0.0340 - acc: 0.9976\n",
            "Epoch 8/10\n",
            "1258/1258 [==============================] - 275s 219ms/step - loss: 0.0342 - acc: 0.9976\n",
            "Epoch 9/10\n",
            "1258/1258 [==============================] - 276s 220ms/step - loss: 0.0332 - acc: 0.9977\n",
            "Epoch 10/10\n",
            "1258/1258 [==============================] - 275s 219ms/step - loss: 0.0327 - acc: 0.9978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_V9D4-TTsCiM",
        "colab_type": "code",
        "outputId": "d15e193b-55a7-4e4d-ff1d-6935d85f37e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r10/10 [==============================] - 1s 67ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['allwaste lt o to acquire firm allwaste inc said it entered into an agreement in principle to buy co a privately held company that in exchange for about 1 3 mln shares or allwaste common',\n",
              " 'allwaste said that earned about one mln dlrs before taxes last year and had about 1 5 mln dlrs in pre tax income for the nine months ended september 30',\n",
              " 'fed data suggest no change in monetary policy new u s banking data suggest the federal reserve is monetary policy along a steady path and is not any imminent change of course economists said',\n",
              " \"but they also said that if money supply growth remains weak as this week ' s anticipating eight billion dlr m 1 decline suggests it may this could influence the fed to its credit and move toward a more monetary policy\",\n",
              " 'a reuter survey of 17 money market economists produced a forecast of a 600 mln dlr m 1 decline for the week ended june 8 with estimates ranging from a gain of one billion dlrs to a decline of four billion',\n",
              " 'instead m 1 fell eight billion dlrs to 745 7 billion dlrs at a seasonally adjusted annual rate',\n",
              " \"coming on the of a 4 3 billion consumption in m 1 for the week ended june 1 this means the nation ' s money supply has fallen more than 12 billion dlrs in the past two weeks economists said\",\n",
              " 'm 1 has hit an air of weakness said bill sullivan of dean witter reynolds inc',\n",
              " 'while m 1 may have lost its significance as an indicator of economic growth sullivan said fed officials might be concerned the latest drop in m 1 means another month of sluggish growth in the broader monetary aggregates m 2 and m 3 which are seen as better of economic growth',\n",
              " \"latest monthly m 2 and m 3 data showed that as of may both measures were growing at rates below the bottom of the fed ' s 5 1 2 to 8 1 2 pct target ranges\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "koH_AU6hsCiT",
        "colab_type": "code",
        "outputId": "0c9d0baa-c5ac-46a5-801b-06b846e8f0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['allwaste lt o to acquire firm allwaste inc said it entered into an agreement in principle to buy co a privately held company that in exchange for about 1 3 mln shares or allwaste common',\n",
              " 'allwaste said that earned about one mln dlrs before taxes last year and had about 1 5 mln dlrs in pre tax income for the nine months ended september 30',\n",
              " 'fed data suggest no change in monetary policy new u s banking data suggest the federal reserve is monetary policy along a steady path and is not any imminent change of course economists said',\n",
              " \"but they also said that if money supply growth remains weak as this week ' s unexpected eight billion dlr m 1 decline suggests it may this could influence the fed to its credit and move toward a more monetary policy\",\n",
              " 'a reuter survey of 17 money market economists produced a forecast of a 600 mln dlr m 1 decline for the week ended june 8 with estimates ranging from a gain of one billion dlrs to a decline of four billion',\n",
              " 'instead m 1 fell eight billion dlrs to 745 7 billion dlrs at a seasonally adjusted annual rate',\n",
              " \"coming on the of a 4 3 billion decrease in m 1 for the week ended june 1 this means the nation ' s money supply has fallen more than 12 billion dlrs in the past two weeks economists said\",\n",
              " 'm 1 has hit an air of weakness said bill sullivan of dean witter reynolds inc',\n",
              " 'while m 1 may have lost its significance as an indicator of economic growth sullivan said fed officials might be concerned the latest drop in m 1 means another month of sluggish growth in the broader monetary aggregates m 2 and m 3 which are seen as better of economic growth',\n",
              " \"latest monthly m 2 and m 3 data showed that as of may both measures were growing at rates below the bottom of the fed ' s 5 1 2 to 8 1 2 pct target ranges\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "7qnApK7Rs_Vs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.engine import Layer, InputSpec\n",
        "from keras import regularizers, initializers, constraints\n",
        "from keras import backend as K\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    :param kwargs:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    refer https://github.com/fchollet/keras/issues/4962\n",
        "    refer https://gist.github.com/rmdort/596e75e864295365798836d9e8636033\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.kernel = self.add_weight((input_shape[2], 1,),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        # word context vector uw\n",
        "        self.u = self.add_weight((input_shape[1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # in the paper refer equations (5) on page 3\n",
        "        # (batch, time_steps, 40) x (40, 1)\n",
        "        W_w_dot_h_it =  K.dot(x, self.kernel) # (batch, 40, 1)\n",
        "        W_w_dot_h_it = K.squeeze(W_w_dot_h_it, -1) # (batch, 40)\n",
        "        W_w_dot_h_it = W_w_dot_h_it + self.b # (batch, 40) + (40,)\n",
        "        uit = K.tanh(W_w_dot_h_it) # (batch, 40)\n",
        "\n",
        "        # in the paper refer equations (6) on page 3\n",
        "        uit_dot_uw = uit * self.u # (batch, 40) * (40, 1) => (batch, 1)\n",
        "        ait = K.exp(uit_dot_uw) # (batch, 1)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx()) #(batch, 40)\n",
        "            ait = mask*ait #(batch, 40) * (batch, 40, )\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number epsilon to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        # sentence vector si is returned\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Shape transformation logic so Keras can infer output shape\n",
        "        \"\"\"\n",
        "        return (input_shape[0], input_shape[-1],)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VpddH74HAdKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import reuters\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing import sequence \n",
        "from itertools import chain\n",
        "\n",
        "categories = [(cat, len(reuters.fileids(categories=cat))) for cat in reuters.categories()]\n",
        "topn = [cat for cat, _ in sorted(categories, key=lambda x: -x[1])[:5]]\n",
        "\n",
        "def iter_labels(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                cat = reuters.categories(fid)[0]\n",
        "                if cat in topn:\n",
        "                  yield cat\n",
        "labels_train = np.array(list(iter_labels('train')))\n",
        "labels_test = np.array(list(iter_labels('test')))\n",
        "\n",
        "def iter_sents(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                cat = reuters.categories(fid)[0]\n",
        "                if cat in topn:\n",
        "                  yield \" \".join(sent)\n",
        "data_train = np.array(list(iter_sents('train')))\n",
        "data_test = np.array(list(iter_sents('test')))\n",
        "\n",
        "index_train = np.arange(data_train.shape[0])\n",
        "index_test = np.arange(data_test.shape[0])\n",
        "\n",
        "np.random.seed(1)\n",
        "np.random.shuffle(index_train)\n",
        "np.random.shuffle(index_test)\n",
        "\n",
        "n_train = 200\n",
        "n_test = 100\n",
        "\n",
        "labels_train = labels_train[index_train][:n_train]\n",
        "data_train = data_train[index_train][:n_train]\n",
        "\n",
        "labels_test = labels_test[index_test][:n_test]\n",
        "data_test = data_test[index_test][:n_test]\n",
        "\n",
        "le = LabelEncoder().fit(topn)\n",
        "y_train = le.transform(labels_train)\n",
        "y_test = le.transform(labels_test)\n",
        "\n",
        "y_train_hat = to_categorical(y_train, le.classes_.shape[0])\n",
        "y_test_hat = to_categorical(y_test, le.classes_.shape[0])\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train)\n",
        "X_test = tokenizer.texts_to_sequences(data_test)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LEN)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQeysmzpBhrF",
        "colab_type": "code",
        "outputId": "3cba7c67-8ee3-45bb-dd94-af07b4e1fa92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model\n",
        "copy = clone_model(model)\n",
        "#encoder = AttentionWithContext()(copy.get_layer('tempconv1').output)\n",
        "encoder = Lambda(lambda x: x[:, -1, :])(copy.get_layer('tempconv1').output)\n",
        "classification_layer = Dense(le.classes_.shape[0], activation='softmax')(encoder)\n",
        "clfr = Model(copy.input, classification_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "clfr.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "#print(clfr.summary())\n",
        "history = clfr.fit(X_train, y_train_hat,\n",
        "                   validation_data=(X_test, y_test_hat), shuffle=True, epochs=10, batch_size=32)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_test_pred = np.argmax(clfr.predict(X_test, verbose=1), axis=1)\n",
        "y_train_pred = np.argmax(clfr.predict(X_train, verbose=1), axis=1)\n",
        "print(\"TRAINING REPORT\")\n",
        "print(classification_report(y_train, y_train_pred, target_names=le.classes_))\n",
        "print(\"VALIDATION REPORT\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=le.classes_))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 14s 69ms/step - loss: 1.5582 - acc: 0.3150 - val_loss: 1.4462 - val_acc: 0.3300\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.3115 - acc: 0.4700 - val_loss: 1.5106 - val_acc: 0.3800\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.1392 - acc: 0.5100 - val_loss: 1.3796 - val_acc: 0.3700\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.9598 - acc: 0.6650 - val_loss: 1.4328 - val_acc: 0.4400\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.7066 - acc: 0.7350 - val_loss: 1.4366 - val_acc: 0.4500\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4116 - acc: 0.9300 - val_loss: 1.8957 - val_acc: 0.4600\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3776 - acc: 0.8950 - val_loss: 2.3900 - val_acc: 0.4600\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1617 - acc: 0.9600 - val_loss: 2.0251 - val_acc: 0.4000\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.0693 - acc: 0.9950 - val_loss: 2.1051 - val_acc: 0.4900\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.1394 - acc: 0.9600 - val_loss: 2.1339 - val_acc: 0.4300\n",
            "100/100 [==============================] - 5s 52ms/step\n",
            "200/200 [==============================] - 1s 3ms/step\n",
            "TRAINING REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.99      1.00      0.99        79\n",
            "       crude       1.00      1.00      1.00        31\n",
            "        earn       1.00      0.98      0.99        54\n",
            "       grain       1.00      1.00      1.00        17\n",
            "    money-fx       1.00      1.00      1.00        19\n",
            "\n",
            "   micro avg       0.99      0.99      0.99       200\n",
            "   macro avg       1.00      1.00      1.00       200\n",
            "weighted avg       1.00      0.99      0.99       200\n",
            "\n",
            "VALIDATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.49      0.63      0.55        35\n",
            "       crude       0.38      0.50      0.43        18\n",
            "        earn       0.61      0.34      0.44        32\n",
            "       grain       0.00      0.00      0.00         9\n",
            "    money-fx       0.17      0.17      0.17         6\n",
            "\n",
            "   micro avg       0.43      0.43      0.43       100\n",
            "   macro avg       0.33      0.33      0.32       100\n",
            "weighted avg       0.44      0.43      0.42       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OnIhw_tqNr77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "4910b536-343b-410f-ab54-3be0cfa7c6b1"
      },
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(MAX_SEQUENCE_LEN,))\n",
        "encoder = None\n",
        "if not USE_GLOVE:\n",
        "  encoder = Embedding(MAX_NUM_WORDS, embedding_size)(input_layer)\n",
        "else:\n",
        "  encoder = Embedding(num_words, embdedding_dim, \n",
        "                      input_length=MAX_SEQUENCE_LEN, \n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      trainable=True)(input_layer)\n",
        "encoder = encoders[encoder_type](encoder)\n",
        "#encoder = AttentionWithContext()(encoder)\n",
        "encoder = Lambda(lambda x: x[:, -1, :])(encoder)\n",
        "classification_layer = Dense(le.classes_.shape[0], activation='softmax')(encoder)\n",
        "clfr2 = Model(input_layer, classification_layer)\n",
        "optimizer = optimizers.Adam(lr=0.001, clipnorm=0.1)\n",
        "clfr2.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "#print(clfr.summary())\n",
        "history = clfr2.fit(X_train, y_train_hat,\n",
        "                   validation_data=(X_test, y_test_hat), shuffle=True, epochs=10, batch_size=32)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "y_test_pred = np.argmax(clfr2.predict(X_test, verbose=1), axis=1)\n",
        "y_train_pred = np.argmax(clfr2.predict(X_train, verbose=1), axis=1)\n",
        "print(\"TRAINING REPORT\")\n",
        "print(classification_report(y_train, y_train_pred, target_names=le.classes_))\n",
        "print(\"VALIDATION REPORT\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=le.classes_))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 200 samples, validate on 100 samples\n",
            "Epoch 1/10\n",
            "200/200 [==============================] - 14s 70ms/step - loss: 1.8311 - acc: 0.2500 - val_loss: 1.7896 - val_acc: 0.2300\n",
            "Epoch 2/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.4612 - acc: 0.4150 - val_loss: 1.7072 - val_acc: 0.3100\n",
            "Epoch 3/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.2399 - acc: 0.5100 - val_loss: 1.6310 - val_acc: 0.3200\n",
            "Epoch 4/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 1.0735 - acc: 0.5700 - val_loss: 1.6036 - val_acc: 0.3400\n",
            "Epoch 5/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.9271 - acc: 0.6600 - val_loss: 1.5907 - val_acc: 0.3600\n",
            "Epoch 6/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.7991 - acc: 0.7350 - val_loss: 1.5952 - val_acc: 0.3600\n",
            "Epoch 7/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.6826 - acc: 0.8050 - val_loss: 1.6068 - val_acc: 0.3500\n",
            "Epoch 8/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.5765 - acc: 0.8400 - val_loss: 1.6370 - val_acc: 0.4000\n",
            "Epoch 9/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.4732 - acc: 0.9000 - val_loss: 1.6643 - val_acc: 0.4100\n",
            "Epoch 10/10\n",
            "200/200 [==============================] - 2s 8ms/step - loss: 0.3785 - acc: 0.9250 - val_loss: 1.7125 - val_acc: 0.3900\n",
            "100/100 [==============================] - 5s 53ms/step\n",
            "200/200 [==============================] - 1s 3ms/step\n",
            "TRAINING REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.94      0.97      0.96        79\n",
            "       crude       1.00      0.97      0.98        31\n",
            "        earn       0.94      0.93      0.93        54\n",
            "       grain       0.81      0.76      0.79        17\n",
            "    money-fx       0.95      0.95      0.95        19\n",
            "\n",
            "   micro avg       0.94      0.94      0.94       200\n",
            "   macro avg       0.93      0.92      0.92       200\n",
            "weighted avg       0.94      0.94      0.94       200\n",
            "\n",
            "VALIDATION REPORT\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.37      0.49      0.42        35\n",
            "       crude       0.27      0.17      0.21        18\n",
            "        earn       0.51      0.56      0.54        32\n",
            "       grain       0.00      0.00      0.00         9\n",
            "    money-fx       0.14      0.17      0.15         6\n",
            "\n",
            "   micro avg       0.39      0.39      0.39       100\n",
            "   macro avg       0.26      0.28      0.26       100\n",
            "weighted avg       0.35      0.39      0.37       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kmcGa-xdNNpR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}