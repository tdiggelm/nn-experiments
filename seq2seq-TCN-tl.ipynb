{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-TCN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "W1_uV8WoFXQu",
        "colab_type": "code",
        "outputId": "285d7b30-f9a1-4e65-e240-9aacaddfbb83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/My\\ Drive/nn_output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xscbWz8wGH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "OUTPUTDIR='/content/drive/My Drive/nn_output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrCPRQxBFJqP",
        "colab_type": "code",
        "outputId": "c96e9714-ef23-4962-ee21-3b4bf134a511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-TCN\n",
        "\n",
        "from keras.layers import (Bidirectional, Dense, Embedding, Input, Lambda, InputLayer, Reshape\n",
        "                          , LSTM, RepeatVector, TimeDistributed, Flatten, Layer)\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from tcn import TCN\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import numpy as np\n",
        "from nltk.corpus import reuters\n",
        "from itertools import chain\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os.path\n",
        "import glob\n",
        "\n",
        "USE_GLOVE = True\n",
        "MAX_SEQUENCE_LEN = 100\n",
        "MAX_NUM_WORDS = 10000"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-TCN\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/bc/dcbdc24d80229022333150f42ff88ddf4c6793568f711a0d6fc1e83b102e/keras_tcn-2.3.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (2.8.0)\n",
            "Installing collected packages: keras-TCN\n",
            "Successfully installed keras-TCN-2.3.5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5lh2ZJRsChy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(fillvalue=fillvalue, *args)\n",
        "\n",
        "class ReutersGenerator():\n",
        "    def __init__(self, max_seq_length=250, num_words=5000):\n",
        "        self.tok = Tokenizer(num_words=num_words)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_words = num_words\n",
        "    \n",
        "    def _gen_sents(self, fids):\n",
        "        return (' '.join(sent) for fid in fids for sent in reuters.sents(fid))\n",
        "    \n",
        "    def fit(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        self.tok.fit_on_texts(self._gen_sents(fids))\n",
        "        return self\n",
        "\n",
        "    def count(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        return sum(1 for _ in self._gen_sents(fids))\n",
        "    \n",
        "    def inverse_transform(self, X):\n",
        "        return self.tok.sequences_to_texts(X)\n",
        "    \n",
        "    def generate_pairs(self, fid_startswith='train', bs=32, \n",
        "                         max_seq_len=250, forever=True, shuffle=True):\n",
        "        fids_in = np.array([fid for fid in reuters.fileids() if fid.startswith(fid_startswith)])\n",
        "        index = np.arange(fids_in.shape[0])\n",
        "        while True:\n",
        "            np.random.shuffle(index)\n",
        "            fids = fids_in[index]\n",
        "            sents = self._gen_sents(fids)\n",
        "            for batch in grouper(sents, bs):\n",
        "                seqs = self.tok.texts_to_sequences_generator(text for text in batch if text)\n",
        "                X = pad_sequences(list(seqs), self.max_seq_length)\n",
        "                yield X, to_categorical(X, self.num_words)\n",
        "            if not forever:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSZgXz1tHKqk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuters_gen = ReutersGenerator(\n",
        "    num_words=MAX_NUM_WORDS, max_seq_length=MAX_SEQUENCE_LEN).fit()\n",
        "n_train = reuters_gen.count('train')\n",
        "#n_test = reuters_gen.count('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6v0kPz67sPl",
        "colab_type": "code",
        "outputId": "fc471d02-3bfe-4a3d-f525-d9fa362a2f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "  !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "  !unzip \"glove.6B.zip\"\n",
        "\n",
        "# get glove coeff matrix\n",
        "embeddings_index = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# prepare pre-learned embedding matrix\n",
        "embdedding_dim = 100\n",
        "word_index = reuters_gen.tok.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-16 07:27:49--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2019-01-16 07:27:49--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  3.47MB/s    in 4m 9s   \n",
            "\n",
            "2019-01-16 07:31:58 (3.31 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "der9TGExhmUz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras.backend as K\n",
        "import keras.layers\n",
        "from keras import optimizers\n",
        "from keras.engine.topology import Layer\n",
        "from keras.layers import Activation, Lambda\n",
        "from keras.layers import Conv1D, SpatialDropout1D\n",
        "from keras.layers import Convolution1D, Dense\n",
        "from keras.models import Input, Model\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "def channel_normalization(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\" Normalize a layer to the maximum activation\n",
        "    This keeps a layers values between zero and one.\n",
        "    It helps with relu's unbounded activation\n",
        "    Args:\n",
        "        x: The layer to normalize\n",
        "    Returns:\n",
        "        A maximal normalized layer\n",
        "    \"\"\"\n",
        "    max_values = K.max(K.abs(x), 2, keepdims=True) + 1e-5\n",
        "    out = x / max_values\n",
        "    return out\n",
        "\n",
        "\n",
        "def wave_net_activation(x):\n",
        "    # type: (Layer) -> Layer\n",
        "    \"\"\"This method defines the activation used for WaveNet\n",
        "    described in https://deepmind.com/blog/wavenet-generative-model-raw-audio/\n",
        "    Args:\n",
        "        x: The layer we want to apply the activation to\n",
        "    Returns:\n",
        "        A new layer with the wavenet activation applied\n",
        "    \"\"\"\n",
        "    tanh_out = Activation('tanh')(x)\n",
        "    sigm_out = Activation('sigmoid')(x)\n",
        "    return keras.layers.multiply([tanh_out, sigm_out])\n",
        "\n",
        "\n",
        "def residual_block(x, s, i, activation, nb_filters, kernel_size, padding, dropout_rate=0, name=''):\n",
        "    # type: (Layer, int, int, str, int, int, str, float, str) -> Tuple[Layer, Layer]\n",
        "    \"\"\"Defines the residual block for the WaveNet TCN\n",
        "    Args:\n",
        "        x: The previous layer in the model\n",
        "        s: The stack index i.e. which stack in the overall TCN\n",
        "        i: The dilation power of 2 we are using for this residual block\n",
        "        activation: The name of the type of activation to use\n",
        "        nb_filters: The number of convolutional filters to use in this block\n",
        "        kernel_size: The size of the convolutional kernel\n",
        "        padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "        name: Name of the model. Useful when having multiple TCN.\n",
        "    Returns:\n",
        "        A tuple where the first element is the residual model layer, and the second\n",
        "        is the skip connection.\n",
        "    \"\"\"\n",
        "\n",
        "    original_x = x\n",
        "    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size,\n",
        "                  dilation_rate=i, padding=padding,\n",
        "                  name=name + '_d_%s_conv_%d_tanh_s%d' % (padding, i, s))(x)\n",
        "    if activation == 'norm_relu':\n",
        "        x = Activation('relu')(conv)\n",
        "        x = Lambda(channel_normalization)(x)\n",
        "    elif activation == 'wavenet':\n",
        "        x = wave_net_activation(conv)\n",
        "    else:\n",
        "        x = Activation(activation)(conv)\n",
        "\n",
        "    x = SpatialDropout1D(dropout_rate, name=name + '_spatial_dropout1d_%d_s%d_%f' % (i, s, dropout_rate))(x)\n",
        "\n",
        "    # 1x1 conv.\n",
        "    x = Convolution1D(nb_filters, 1, padding='same')(x)\n",
        "    res_x = keras.layers.add([original_x, x])\n",
        "    return res_x, x\n",
        "\n",
        "\n",
        "def process_dilations(dilations):\n",
        "    def is_power_of_two(num):\n",
        "        return num != 0 and ((num & (num - 1)) == 0)\n",
        "\n",
        "    if all([is_power_of_two(i) for i in dilations]):\n",
        "        return dilations\n",
        "\n",
        "    else:\n",
        "        new_dilations = [2 ** i for i in dilations]\n",
        "        # print(f'Updated dilations from {dilations} to {new_dilations} because of backwards compatibility.')\n",
        "        return new_dilations\n",
        "\n",
        "\n",
        "class TCN(Layer):\n",
        "    \"\"\"Creates a TCN layer.\n",
        "        Input shape:\n",
        "            A tensor of shape (batch_size, timesteps, input_dim).\n",
        "        Args:\n",
        "            nb_filters: The number of filters to use in the convolutional layers.\n",
        "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "            nb_stacks : The number of stacks of residual blocks to use.\n",
        "            activation: The activations to use (norm_relu, wavenet, relu...).\n",
        "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
        "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "            name: Name of the model. Useful when having multiple TCN.\n",
        "        Returns:\n",
        "            A TCN layer.\n",
        "        \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 nb_filters=64,\n",
        "                 kernel_size=2,\n",
        "                 nb_stacks=1,\n",
        "                 dilations=[1, 2, 4, 8, 16, 32],\n",
        "                 activation='norm_relu',\n",
        "                 padding='causal',\n",
        "                 use_skip_connections=True,\n",
        "                 dropout_rate=0.0,\n",
        "                 return_sequences=True,\n",
        "                 name='tcn'):\n",
        "        self.name = name\n",
        "        self.return_sequences = return_sequences\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.use_skip_connections = use_skip_connections\n",
        "        self.activation = activation\n",
        "        self.dilations = dilations\n",
        "        self.nb_stacks = nb_stacks\n",
        "        self.kernel_size = kernel_size\n",
        "        self.nb_filters = nb_filters\n",
        "        self.padding = padding\n",
        "\n",
        "        if padding != 'causal' and padding != 'same':\n",
        "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
        "\n",
        "        if not isinstance(nb_filters, int):\n",
        "            print('An interface change occurred after the version 2.1.2.')\n",
        "            print('Before: tcn.TCN(i, return_sequences=False, ...)')\n",
        "            print('Now should be: tcn.TCN(return_sequences=False, ...)(i)')\n",
        "            print('Second solution is to pip install keras-tcn==2.1.2 to downgrade.')\n",
        "            raise Exception()\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        x = Convolution1D(self.nb_filters, 1, padding=self.padding, name=self.name + '_initial_conv')(x)\n",
        "        skip_connections = []\n",
        "        for s in range(self.nb_stacks):\n",
        "            for i in self.dilations:\n",
        "                x, skip_out = residual_block(x, s, i, self.activation, self.nb_filters,\n",
        "                                             self.kernel_size, self.padding, self.dropout_rate, name=self.name)\n",
        "                skip_connections.append(skip_out)\n",
        "        if self.use_skip_connections:\n",
        "            x = keras.layers.add(skip_connections)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        if not self.return_sequences:\n",
        "            output_slice_index = -1\n",
        "            x = Lambda(lambda tt: tt[:, output_slice_index, :])(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def compiled_tcn(num_feat,  # type: int\n",
        "                 num_classes,  # type: int\n",
        "                 nb_filters,  # type: int\n",
        "                 kernel_size,  # type: int\n",
        "                 dilations,  # type: List[int]\n",
        "                 nb_stacks,  # type: int\n",
        "                 max_len,  # type: int\n",
        "                 activation='norm_relu',  # type: str\n",
        "                 padding='causal',  # type: str\n",
        "                 use_skip_connections=True,  # type: bool\n",
        "                 return_sequences=True,\n",
        "                 regression=False,  # type: bool\n",
        "                 dropout_rate=0.05,  # type: float\n",
        "                 name='tcn'  # type: str\n",
        "                 ):\n",
        "    # type: (...) -> keras.Model\n",
        "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
        "    Args:\n",
        "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
        "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
        "        nb_filters: The number of filters to use in the convolutional layers.\n",
        "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
        "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
        "        nb_stacks : The number of stacks of residual blocks to use.\n",
        "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
        "        activation: The activations to use.\n",
        "        padding: The padding to use in the convolutional layers.\n",
        "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual block.\n",
        "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
        "        regression: Whether the output should be continuous or discrete.\n",
        "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
        "        name: Name of the model. Useful when having multiple TCN.\n",
        "    Returns:\n",
        "        A compiled keras TCN.\n",
        "    \"\"\"\n",
        "\n",
        "    dilations = process_dilations(dilations)\n",
        "\n",
        "    input_layer = Input(shape=(max_len, num_feat))\n",
        "\n",
        "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, activation,\n",
        "            padding, use_skip_connections, dropout_rate, return_sequences, name)(input_layer)\n",
        "\n",
        "    print('x.shape=', x.shape)\n",
        "\n",
        "    if not regression:\n",
        "        # classification\n",
        "        x = Dense(num_classes)(x)\n",
        "        x = Activation('softmax')(x)\n",
        "        output_layer = x\n",
        "        print(f'model.x = {input_layer.shape}')\n",
        "        print(f'model.y = {output_layer.shape}')\n",
        "        model = Model(input_layer, output_layer)\n",
        "\n",
        "        # https://github.com/keras-team/keras/pull/11373\n",
        "        # It's now in Keras@master but still not available with pip.\n",
        "        # TODO To remove later.\n",
        "        def accuracy(y_true, y_pred):\n",
        "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
        "            if K.ndim(y_true) == K.ndim(y_pred):\n",
        "                y_true = K.squeeze(y_true, -1)\n",
        "            # convert dense predictions to labels\n",
        "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
        "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
        "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
        "\n",
        "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
        "        model.compile(adam, loss='sparse_categorical_crossentropy', metrics=[accuracy])\n",
        "        print('Adam with norm clipping.')\n",
        "    else:\n",
        "        # regression\n",
        "        x = Dense(1)(x)\n",
        "        x = Activation('linear')(x)\n",
        "        output_layer = x\n",
        "        print(f'model.x = {input_layer.shape}')\n",
        "        print(f'model.y = {output_layer.shape}')\n",
        "        model = Model(input_layer, output_layer)\n",
        "        adam = optimizers.Adam(lr=0.002, clipnorm=1.)\n",
        "        model.compile(adam, loss='mean_squared_error')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXHnYTNY7zsD",
        "colab_type": "code",
        "outputId": "c22b86ea-4a05-4cf6-9fda-ef9eddb377a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "cell_type": "code",
      "source": [
        "kernel_size = 3\n",
        "n_dilations = 8\n",
        "n_hidden = 256\n",
        "embedding_size = 100\n",
        "dropout=0.4\n",
        "\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LEN,))\n",
        "encoder = None\n",
        "if not USE_GLOVE:\n",
        "  encoder = Embedding(MAX_NUM_WORDS, embedding_size)(input_layer)\n",
        "else:\n",
        "  encoder = Embedding(num_words, embdedding_dim, \n",
        "                      input_length=MAX_SEQUENCE_LEN, \n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      trainable=True)(input_layer)\n",
        "encoder = TemporalConvNet(name='tempconv1', return_sequences=True,\n",
        "              kernel_size=kernel_size,\n",
        "              dilations=[2**n for n in range(n_dilations)],\n",
        "              nb_filters=n_hidden,\n",
        "              nb_stacks=1,\n",
        "              dropout_rate=dropout)(encoder)\n",
        "#encoder = TCN(name='tempconv2', return_sequences=True,\n",
        "#              kernel_size=kernel_size,\n",
        "#              dilations=[2**n for n in range(n_dilations)],\n",
        "#              nb_filters=n_hidden,\n",
        "#              nb_stacks=1,\n",
        "#              dropout_rate=dropout)(encoder)\n",
        "output_layer = TimeDistributed(Dense(MAX_NUM_WORDS, activation='softmax'))(encoder)\n",
        "model = Model(input_layer, output_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "print(model.summary())"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-138-b2f8c0cf205f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m               \u001b[0mnb_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m               \u001b[0mnb_stacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m               dropout_rate=dropout)(encoder)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#encoder = TCN(name='tempconv2', return_sequences=True,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#              kernel_size=kernel_size,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Keyword argument not understood:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'return_sequences')"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "1MR8JitksCiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basename = 'seq2seq-TCN-model-double'\n",
        "outfname = os.path.join(\n",
        "    OUTPUTDIR,\n",
        "    basename + '-ep{epoch:02d}.hdf5')\n",
        "cp = ModelCheckpoint(\n",
        "    outfname,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7AgCeZ3E_8D",
        "colab_type": "code",
        "outputId": "7c29c9b8-466a-4b12-f367-edaebeee8112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "TRAIN_MODEL = True\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "if TRAIN_MODEL:\n",
        "  history = model.fit_generator(reuters_gen.generate_pairs('train', bs=BATCH_SIZE),\n",
        "      #validation_data=reuters_gen.generate_pairs('test', bs=BATCH_SIZE),\n",
        "      steps_per_epoch=n_train//BATCH_SIZE,\n",
        "      #validation_steps=n_test//BATCH_SIZE,\n",
        "      epochs=EPOCHS, shuffle=True, callbacks=[cp])\n",
        "else:\n",
        "  list_of_files = glob.glob(os.path.join(OUTPUTDIR, basename + '*.hdf5'))\n",
        "  list_of_files = sorted(list_of_files, key=os.path.getctime)\n",
        "  assert(len(list_of_files) > 0)\n",
        "  model = load_model(list_of_files[-1])\n",
        "  print('Loaded model from \\'%s\\'' % list_of_files[-1])"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1258/1258 [==============================] - 270s 215ms/step - loss: 0.3265 - acc: 0.9597\n",
            "Epoch 2/10\n",
            "1258/1258 [==============================] - 265s 211ms/step - loss: 0.0556 - acc: 0.9941\n",
            "Epoch 3/10\n",
            "1258/1258 [==============================] - 265s 210ms/step - loss: 0.0390 - acc: 0.9965\n",
            "Epoch 4/10\n",
            "1258/1258 [==============================] - 265s 210ms/step - loss: 0.0360 - acc: 0.9970\n",
            "Epoch 5/10\n",
            "1258/1258 [==============================] - 265s 210ms/step - loss: 0.0344 - acc: 0.9974\n",
            "Epoch 6/10\n",
            "1258/1258 [==============================] - 265s 210ms/step - loss: 0.0337 - acc: 0.9975\n",
            "Epoch 7/10\n",
            "1258/1258 [==============================] - 267s 212ms/step - loss: 0.0330 - acc: 0.9976\n",
            "Epoch 8/10\n",
            "1258/1258 [==============================] - 267s 213ms/step - loss: 0.0321 - acc: 0.9978\n",
            "Epoch 9/10\n",
            "1258/1258 [==============================] - 266s 211ms/step - loss: 0.0320 - acc: 0.9978\n",
            "Epoch 10/10\n",
            "1258/1258 [==============================] - 264s 210ms/step - loss: 0.0310 - acc: 0.9979\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsIoTBfwnnqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test, X_test_hat = next(reuters_gen.generate_pairs('test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_V9D4-TTsCiM",
        "colab_type": "code",
        "outputId": "18691b23-c77f-46f2-e08c-523ac2366266",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(np.argmax(model.predict(X_test[:10], verbose=1), axis=2))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r10/10 [==============================] - 0s 6ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"cotton stocks cotton stocks deliverable on the new york cotton exchange no 2 cotton futures contract as of april 8 were reported at 34 661 bales down 421 bales from the previous day ' s figure\",\n",
              " 'there were no bales awaiting review and 1 218 bales awaiting',\n",
              " 'top officials arrive at treasury for g 5 talks top officials of leading industrial nations arrived at the u s treasury main building to begin a meeting of the group of five',\n",
              " 'officials seen by reuter included west german finance minister gerhard stoltenberg and bundesbank president karl horner poehl french finance minister edouard balladur and his central banker jacques de',\n",
              " \"also seen were japanese finance minister kiichi miyazawa and japan ' s central bank governor satoshi sumita and british chancellor of the exchequer and central bank governor robin leigh pemberton\",\n",
              " 'there was no immediate sign of italian or canadian officials',\n",
              " 'monetary sources have said a fully meeting of the group of seven is expected to begin around 3 p m local time gmt and last at least until 6 p m gmt when a communique is expected to be issued',\n",
              " 'italian sources said italian worked finance minister met treasury secretary james baker last night',\n",
              " 'at those talks baker apparently convinced who declined to attend the february meeting of the group of seven in paris that italy would participate fully in any meaningful decisions',\n",
              " 'grain ships waiting at new orleans ten grain ships were loading and 14 were waiting to load at new orleans elevators trade sources said']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "metadata": {
        "id": "koH_AU6hsCiT",
        "colab_type": "code",
        "outputId": "39ce912e-c32d-42fa-85c1-d99f857160c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(X_test[:10])"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"cotton stocks cotton stocks deliverable on the new york cotton exchange no 2 cotton futures contract as of april 8 were reported at 34 661 bales down 421 bales from the previous day ' s figure\",\n",
              " 'there were no bales awaiting review and 1 218 bales awaiting',\n",
              " 'top officials arrive at treasury for g 5 talks top officials of leading industrial nations arrived at the u s treasury main building to begin a meeting of the group of five',\n",
              " 'officials seen by reuter included west german finance minister gerhard stoltenberg and bundesbank president karl otto poehl french finance minister edouard balladur and his central banker jacques de',\n",
              " \"also seen were japanese finance minister kiichi miyazawa and japan ' s central bank governor satoshi sumita and british chancellor of the exchequer and central bank governor robin leigh pemberton\",\n",
              " 'there was no immediate sign of italian or canadian officials',\n",
              " 'monetary sources have said a fully meeting of the group of seven is expected to begin around 3 p m local time gmt and last at least until 6 p m gmt when a communique is expected to be issued',\n",
              " 'italian sources said italian acting finance minister met treasury secretary james baker last night',\n",
              " 'at those talks baker apparently convinced who declined to attend the february meeting of the group of seven in paris that italy would participate fully in any meaningful decisions',\n",
              " 'grain ships waiting at new orleans ten grain ships were loading and 14 were waiting to load at new orleans elevators trade sources said']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "metadata": {
        "id": "7qnApK7Rs_Vs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow.contrib.keras as keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras.engine import Layer, InputSpec\n",
        "from keras import regularizers, initializers, constraints\n",
        "from keras import backend as K\n",
        "\n",
        "class AttentionWithContext(Layer):\n",
        "\n",
        "    \"\"\"\n",
        "    Attention operation, with a context/query vector, for temporal data.\n",
        "    Supports Masking.\n",
        "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
        "    \"Hierarchical Attention Networks for Document Classification\"\n",
        "    by using a context vector to assist the attention\n",
        "    # Input shape\n",
        "        3D tensor with shape: `(samples, steps, features)`.\n",
        "    # Output shape\n",
        "        2D tensor with shape: `(samples, features)`.\n",
        "    :param kwargs:\n",
        "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "    The dimensions are inferred based on the output shape of the RNN.\n",
        "    refer https://github.com/fchollet/keras/issues/4962\n",
        "    refer https://gist.github.com/rmdort/596e75e864295365798836d9e8636033\n",
        "    Example:\n",
        "        model.add(LSTM(64, return_sequences=True))\n",
        "        model.add(AttentionWithContext())\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.u_regularizer = regularizers.get(u_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.u_constraint = constraints.get(u_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWithContext, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.kernel = self.add_weight((input_shape[2], 1,),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "\n",
        "        # word context vector uw\n",
        "        self.u = self.add_weight((input_shape[1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_u'.format(self.name),\n",
        "                                 regularizer=self.u_regularizer,\n",
        "                                 constraint=self.u_constraint)\n",
        "\n",
        "        super(AttentionWithContext, self).build(input_shape)\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # in the paper refer equations (5) on page 3\n",
        "        # (batch, time_steps, 40) x (40, 1)\n",
        "        W_w_dot_h_it =  K.dot(x, self.kernel) # (batch, 40, 1)\n",
        "        W_w_dot_h_it = K.squeeze(W_w_dot_h_it, -1) # (batch, 40)\n",
        "        W_w_dot_h_it = W_w_dot_h_it + self.b # (batch, 40) + (40,)\n",
        "        uit = K.tanh(W_w_dot_h_it) # (batch, 40)\n",
        "\n",
        "        # in the paper refer equations (6) on page 3\n",
        "        uit_dot_uw = uit * self.u # (batch, 40) * (40, 1) => (batch, 1)\n",
        "        ait = K.exp(uit_dot_uw) # (batch, 1)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            mask = K.cast(mask, K.floatx()) #(batch, 40)\n",
        "            ait = mask*ait #(batch, 40) * (batch, 40, )\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number epsilon to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "        ait = K.expand_dims(ait)\n",
        "        weighted_input = x * ait\n",
        "        # sentence vector si is returned\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return input_shape[0], input_shape[-1]\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"Shape transformation logic so Keras can infer output shape\n",
        "        \"\"\"\n",
        "        return (input_shape[0], input_shape[-1],)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VpddH74HAdKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import reuters\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing import sequence \n",
        "from itertools import chain\n",
        "\n",
        "categories = [(cat, len(reuters.fileids(categories=cat))) for cat in reuters.categories()]\n",
        "topn = [cat for cat, _ in sorted(categories, key=lambda x: -x[1])[:5]]\n",
        "\n",
        "fids_train = np.array([fid for fid in reuters.fileids() if fid.startswith('train')])\n",
        "fids_test = np.array([fid for fid in reuters.fileids() if fid.startswith('test')])\n",
        "\n",
        "def iter_labels(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                cat = reuters.categories(fid)[0]\n",
        "                if cat in topn:\n",
        "                  yield cat\n",
        "labels_train = np.array(list(iter_labels('train')))\n",
        "labels_test = np.array(list(iter_labels('test')))\n",
        "\n",
        "le = LabelEncoder().fit(topn)\n",
        "y_train = le.transform(labels_train)\n",
        "y_test = le.transform(labels_test)\n",
        "\n",
        "def iter_sents(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                cat = reuters.categories(fid)[0]\n",
        "                if cat in topn:\n",
        "                  yield \" \".join(sent)\n",
        "data_train = np.array(list(iter_sents('train')))\n",
        "data_test = np.array(list(iter_sents('test')))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train)\n",
        "X_test = tokenizer.texts_to_sequences(data_test)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LEN)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LEN)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TVIJVsHYEpHZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train_hat = to_categorical(y_train, le.classes_.shape[0])\n",
        "y_test_hat = to_categorical(y_test, le.classes_.shape[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HQeysmzpBhrF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2227
        },
        "outputId": "c2e5912c-1b47-4d80-d2b5-7a7889a490c7"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import clone_model\n",
        "copy = clone_model(model)\n",
        "attention = AttentionWithContext()(copy.get_layer('activation_99').output)\n",
        "classification_layer = Dense(le.classes_.shape[0], activation='softmax')(attention)\n",
        "clfr = Model(copy.input, classification_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "clfr.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "print(clfr.summary())"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_6 (Embedding)         (None, 100, 100)     1000100     input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_initial_conv (Conv1D) (None, 100, 256)     25856       embedding_6[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_1_tanh_ (None, 100, 256)     196864      tempconv1_initial_conv[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_1_tanh_s0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_83 (Lambda)              (None, 100, 256)     0           activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_1_s (None, 100, 256)     0           lambda_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_81 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_1_s0_\n",
            "__________________________________________________________________________________________________\n",
            "add_91 (Add)                    (None, 100, 256)     0           tempconv1_initial_conv[0][0]     \n",
            "                                                                 conv1d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_2_tanh_ (None, 100, 256)     196864      add_91[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_2_tanh_s0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_84 (Lambda)              (None, 100, 256)     0           activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_2_s (None, 100, 256)     0           lambda_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_82 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_2_s0_\n",
            "__________________________________________________________________________________________________\n",
            "add_92 (Add)                    (None, 100, 256)     0           add_91[0][0]                     \n",
            "                                                                 conv1d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_4_tanh_ (None, 100, 256)     196864      add_92[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_4_tanh_s0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_85 (Lambda)              (None, 100, 256)     0           activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_4_s (None, 100, 256)     0           lambda_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_83 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_4_s0_\n",
            "__________________________________________________________________________________________________\n",
            "add_93 (Add)                    (None, 100, 256)     0           add_92[0][0]                     \n",
            "                                                                 conv1d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_8_tanh_ (None, 100, 256)     196864      add_93[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_8_tanh_s0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_86 (Lambda)              (None, 100, 256)     0           activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_8_s (None, 100, 256)     0           lambda_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_84 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_8_s0_\n",
            "__________________________________________________________________________________________________\n",
            "add_94 (Add)                    (None, 100, 256)     0           add_93[0][0]                     \n",
            "                                                                 conv1d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_16_tanh (None, 100, 256)     196864      add_94[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_16_tanh_s\n",
            "__________________________________________________________________________________________________\n",
            "lambda_87 (Lambda)              (None, 100, 256)     0           activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_16_ (None, 100, 256)     0           lambda_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_85 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_16_s0\n",
            "__________________________________________________________________________________________________\n",
            "add_95 (Add)                    (None, 100, 256)     0           add_94[0][0]                     \n",
            "                                                                 conv1d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_32_tanh (None, 100, 256)     196864      add_95[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_32_tanh_s\n",
            "__________________________________________________________________________________________________\n",
            "lambda_88 (Lambda)              (None, 100, 256)     0           activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_32_ (None, 100, 256)     0           lambda_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_86 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_32_s0\n",
            "__________________________________________________________________________________________________\n",
            "add_96 (Add)                    (None, 100, 256)     0           add_95[0][0]                     \n",
            "                                                                 conv1d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_64_tanh (None, 100, 256)     196864      add_96[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_64_tanh_s\n",
            "__________________________________________________________________________________________________\n",
            "lambda_89 (Lambda)              (None, 100, 256)     0           activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_64_ (None, 100, 256)     0           lambda_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_87 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_64_s0\n",
            "__________________________________________________________________________________________________\n",
            "add_97 (Add)                    (None, 100, 256)     0           add_96[0][0]                     \n",
            "                                                                 conv1d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_d_causal_conv_128_tan (None, 100, 256)     196864      add_97[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 100, 256)     0           tempconv1_d_causal_conv_128_tanh_\n",
            "__________________________________________________________________________________________________\n",
            "lambda_90 (Lambda)              (None, 100, 256)     0           activation_98[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tempconv1_spatial_dropout1d_128 (None, 100, 256)     0           lambda_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_88 (Conv1D)              (None, 100, 256)     65792       tempconv1_spatial_dropout1d_128_s\n",
            "__________________________________________________________________________________________________\n",
            "add_99 (Add)                    (None, 100, 256)     0           conv1d_81[0][0]                  \n",
            "                                                                 conv1d_82[0][0]                  \n",
            "                                                                 conv1d_83[0][0]                  \n",
            "                                                                 conv1d_84[0][0]                  \n",
            "                                                                 conv1d_85[0][0]                  \n",
            "                                                                 conv1d_86[0][0]                  \n",
            "                                                                 conv1d_87[0][0]                  \n",
            "                                                                 conv1d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_99 (Activation)      (None, 100, 256)     0           add_99[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "attention_with_context_12 (Atte (None, 256)          456         activation_99[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_21 (Dense)                (None, 5)            1285        attention_with_context_12[0][0]  \n",
            "==================================================================================================\n",
            "Total params: 3,128,945\n",
            "Trainable params: 3,128,945\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e38HHu3dBuQr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "2a0370af-b8c9-4a74-c619-f1412ed44c25"
      },
      "cell_type": "code",
      "source": [
        "history = clfr.fit(X_train, y_train_hat,\n",
        "                   validation_data=(X_test, y_test_hat), shuffle=True, epochs=3, batch_size=32)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 23169 samples, validate on 8167 samples\n",
            "Epoch 1/3\n",
            "23169/23169 [==============================] - 80s 3ms/step - loss: 0.8890 - acc: 0.6664 - val_loss: 0.6022 - val_acc: 0.7925\n",
            "Epoch 2/3\n",
            "23169/23169 [==============================] - 71s 3ms/step - loss: 0.4426 - acc: 0.8446 - val_loss: 0.5159 - val_acc: 0.8216\n",
            "Epoch 3/3\n",
            "23169/23169 [==============================] - 72s 3ms/step - loss: 0.3073 - acc: 0.8937 - val_loss: 0.5439 - val_acc: 0.8282\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dVlrJ8SFGUIg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e4ed63c-20ab-4b00-c7d4-8fb408e52fcf"
      },
      "cell_type": "code",
      "source": [
        "y_test_pred = np.argmax(clfr.predict(X_test, verbose=1), axis=1)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8167/8167 [==============================] - 9s 1ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HqZNj_QyaJic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b4a62b5-7eac-43bd-cf2d-17589f8f01f1"
      },
      "cell_type": "code",
      "source": [
        "y_train_pred = np.argmax(clfr.predict(X_train, verbose=1), axis=1)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23169/23169 [==============================] - 16s 695us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5mWYYqKAaPha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "94853d9a-ccb4-40fc-907c-9bf35598fa0a"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_train, y_train_pred, target_names=le.classes_))"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.93      0.96      0.94      8728\n",
            "       crude       0.94      0.88      0.91      3104\n",
            "        earn       0.94      0.94      0.94      7830\n",
            "       grain       0.90      0.92      0.91      1457\n",
            "    money-fx       0.94      0.91      0.92      2050\n",
            "\n",
            "   micro avg       0.93      0.93      0.93     23169\n",
            "   macro avg       0.93      0.92      0.93     23169\n",
            "weighted avg       0.93      0.93      0.93     23169\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MOt1r-gJCOXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "3a2cc50e-56ad-46fa-da87-af5f2d5e9bd6"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_test_pred, target_names=le.classes_))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acq       0.87      0.84      0.85      3593\n",
            "       crude       0.84      0.79      0.81      1354\n",
            "        earn       0.80      0.88      0.84      2133\n",
            "       grain       0.72      0.77      0.74       426\n",
            "    money-fx       0.79      0.70      0.74       661\n",
            "\n",
            "   micro avg       0.83      0.83      0.83      8167\n",
            "   macro avg       0.80      0.80      0.80      8167\n",
            "weighted avg       0.83      0.83      0.83      8167\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r_xseK_tHNo-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}