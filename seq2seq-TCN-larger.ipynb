{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-TCN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "W1_uV8WoFXQu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "977d8b13-318e-4557-953e-a6a1cc5145d3"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/My\\ Drive/nn_output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xscbWz8wGH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "OUTPUTDIR='/content/drive/My Drive/nn_output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrCPRQxBFJqP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "fcd936a7-4dc1-477e-c08d-d7dea99c8893"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-TCN\n",
        "\n",
        "from keras.layers import (Bidirectional, Dense, Embedding, Input, Lambda, InputLayer, Reshape\n",
        "                          , LSTM, RepeatVector, TimeDistributed)\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from tcn import TCN\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from nltk.corpus import reuters\n",
        "from itertools import chain\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os.path\n",
        "import glob"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-TCN in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (2.2.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.6)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.11.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4MSNGaUfsChk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2771
        },
        "outputId": "80af6e9c-8cc7-4fd2-ca4c-ba5e05922a81"
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LEN = 250\n",
        "MAX_NUM_WORDS = 5000\n",
        "\n",
        "kernel_size = 4\n",
        "n_dilations = 5\n",
        "n_hidden = 256\n",
        "embedding_size = 200\n",
        "dropout=0.4\n",
        "\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LEN,))\n",
        "encoder = Embedding(MAX_NUM_WORDS, embedding_size)(input_layer)\n",
        "encoder = TCN(name='latent', return_sequences=False,\n",
        "              kernel_size=kernel_size,\n",
        "              dilations=[2**n for n in range(n_dilations)],\n",
        "              nb_filters=n_hidden,\n",
        "              nb_stacks=1,\n",
        "              dropout_rate=dropout)(encoder)\n",
        "decoder = RepeatVector(MAX_SEQUENCE_LEN, name='decoder')(encoder)\n",
        "decoder = TCN(return_sequences=True,\n",
        "              kernel_size=kernel_size,\n",
        "              dilations=[2**n for n in range(n_dilations)],\n",
        "              nb_filters=n_hidden,\n",
        "              nb_stacks=1,\n",
        "              dropout_rate=dropout)(decoder)\n",
        "output_layer = TimeDistributed(Dense(MAX_NUM_WORDS, activation='softmax'))(decoder)\n",
        "model = Model(input_layer, output_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "print(model.summary())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 250)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 250, 200)     1000000     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "latent_initial_conv (Conv1D)    (None, 250, 256)     51456       embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "latent_d_causal_conv_1_tanh_s0  (None, 250, 256)     262400      latent_initial_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 250, 256)     0           latent_d_causal_conv_1_tanh_s0[0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 250, 256)     0           activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "latent_spatial_dropout1d_1_s0_0 (None, 250, 256)     0           lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 250, 256)     65792       latent_spatial_dropout1d_1_s0_0.4\n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 250, 256)     0           latent_initial_conv[0][0]        \n",
            "                                                                 conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "latent_d_causal_conv_2_tanh_s0  (None, 250, 256)     262400      add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 250, 256)     0           latent_d_causal_conv_2_tanh_s0[0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 250, 256)     0           activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "latent_spatial_dropout1d_2_s0_0 (None, 250, 256)     0           lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 250, 256)     65792       latent_spatial_dropout1d_2_s0_0.4\n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 250, 256)     0           add_1[0][0]                      \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "latent_d_causal_conv_4_tanh_s0  (None, 250, 256)     262400      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 250, 256)     0           latent_d_causal_conv_4_tanh_s0[0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 250, 256)     0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "latent_spatial_dropout1d_4_s0_0 (None, 250, 256)     0           lambda_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 250, 256)     65792       latent_spatial_dropout1d_4_s0_0.4\n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 250, 256)     0           add_2[0][0]                      \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "latent_d_causal_conv_8_tanh_s0  (None, 250, 256)     262400      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 250, 256)     0           latent_d_causal_conv_8_tanh_s0[0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_4 (Lambda)               (None, 250, 256)     0           activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "latent_spatial_dropout1d_8_s0_0 (None, 250, 256)     0           lambda_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 250, 256)     65792       latent_spatial_dropout1d_8_s0_0.4\n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 250, 256)     0           add_3[0][0]                      \n",
            "                                                                 conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "latent_d_causal_conv_16_tanh_s0 (None, 250, 256)     262400      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 250, 256)     0           latent_d_causal_conv_16_tanh_s0[0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_5 (Lambda)               (None, 250, 256)     0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "latent_spatial_dropout1d_16_s0_ (None, 250, 256)     0           lambda_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 250, 256)     65792       latent_spatial_dropout1d_16_s0_0.\n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 250, 256)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "                                                                 conv1d_3[0][0]                   \n",
            "                                                                 conv1d_4[0][0]                   \n",
            "                                                                 conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 250, 256)     0           add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "lambda_6 (Lambda)               (None, 256)          0           activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "decoder (RepeatVector)          (None, 250, 256)     0           lambda_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tcn_initial_conv (Conv1D)       (None, 250, 256)     65792       decoder[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_1_tanh_s0 (Co (None, 250, 256)     262400      tcn_initial_conv[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 250, 256)     0           tcn_d_causal_conv_1_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_7 (Lambda)               (None, 250, 256)     0           activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_1_s0_0.40 (None, 250, 256)     0           lambda_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 250, 256)     65792       tcn_spatial_dropout1d_1_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 250, 256)     0           tcn_initial_conv[0][0]           \n",
            "                                                                 conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_2_tanh_s0 (Co (None, 250, 256)     262400      add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 250, 256)     0           tcn_d_causal_conv_2_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 250, 256)     0           activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_2_s0_0.40 (None, 250, 256)     0           lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 250, 256)     65792       tcn_spatial_dropout1d_2_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 250, 256)     0           add_7[0][0]                      \n",
            "                                                                 conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_4_tanh_s0 (Co (None, 250, 256)     262400      add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 250, 256)     0           tcn_d_causal_conv_4_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_9 (Lambda)               (None, 250, 256)     0           activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_4_s0_0.40 (None, 250, 256)     0           lambda_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_8 (Conv1D)               (None, 250, 256)     65792       tcn_spatial_dropout1d_4_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 250, 256)     0           add_8[0][0]                      \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_8_tanh_s0 (Co (None, 250, 256)     262400      add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 250, 256)     0           tcn_d_causal_conv_8_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_10 (Lambda)              (None, 250, 256)     0           activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_8_s0_0.40 (None, 250, 256)     0           lambda_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_9 (Conv1D)               (None, 250, 256)     65792       tcn_spatial_dropout1d_8_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 250, 256)     0           add_9[0][0]                      \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_16_tanh_s0 (C (None, 250, 256)     262400      add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 250, 256)     0           tcn_d_causal_conv_16_tanh_s0[0][0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_11 (Lambda)              (None, 250, 256)     0           activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_16_s0_0.4 (None, 250, 256)     0           lambda_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_10 (Conv1D)              (None, 250, 256)     65792       tcn_spatial_dropout1d_16_s0_0.400\n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 250, 256)     0           conv1d_6[0][0]                   \n",
            "                                                                 conv1d_7[0][0]                   \n",
            "                                                                 conv1d_8[0][0]                   \n",
            "                                                                 conv1d_9[0][0]                   \n",
            "                                                                 conv1d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 250, 256)     0           add_12[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 250, 5000)    1285000     activation_12[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 5,684,168\n",
            "Trainable params: 5,684,168\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5lh2ZJRsChy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(fillvalue=fillvalue, *args)\n",
        "\n",
        "class ReutersGenerator():\n",
        "    def __init__(self, max_seq_length=250, num_words=5000):\n",
        "        self.tok = Tokenizer(num_words=num_words)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_words = num_words\n",
        "    \n",
        "    def _gen_sents(self, fids):\n",
        "        return (' '.join(sent) for fid in fids for sent in reuters.sents(fid))\n",
        "    \n",
        "    def fit(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        self.tok.fit_on_texts(self._gen_sents(fids))\n",
        "        return self\n",
        "\n",
        "    def count(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        return sum(1 for _ in self._gen_sents(fids))\n",
        "    \n",
        "    def inverse_transform(self, X):\n",
        "        return self.tok.sequences_to_texts(X)\n",
        "    \n",
        "    def generate_pairs(self, fid_startswith='train', bs=32, \n",
        "                         max_seq_len=250, forever=True, shuffle=True):\n",
        "        fids_in = np.array([fid for fid in reuters.fileids() if fid.startswith(fid_startswith)])\n",
        "        index = np.arange(fids_in.shape[0])\n",
        "        while True:\n",
        "            np.random.shuffle(index)\n",
        "            fids = fids_in[index]\n",
        "            sents = self._gen_sents(fids)\n",
        "            for batch in grouper(sents, bs):\n",
        "                seqs = self.tok.texts_to_sequences_generator(text for text in batch if text)\n",
        "                X = pad_sequences(list(seqs), self.max_seq_length)\n",
        "                yield X, to_categorical(X, self.num_words)\n",
        "            if not forever:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSZgXz1tHKqk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuters_gen = ReutersGenerator(\n",
        "    num_words=MAX_NUM_WORDS, max_seq_length=MAX_SEQUENCE_LEN).fit()\n",
        "n_train = reuters_gen.count('train')\n",
        "#n_test = reuters_gen.count('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1MR8JitksCiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basename = 'seq2seq-TCN-model'\n",
        "outfname = os.path.join(\n",
        "    OUTPUTDIR,\n",
        "    basename + '-ep{epoch:02d}.hdf5')\n",
        "cp = ModelCheckpoint(\n",
        "    outfname,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7AgCeZ3E_8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ee6c0013-8267-4637-b018-2ce68707eaad"
      },
      "cell_type": "code",
      "source": [
        "TRAIN_MODEL = True\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 10\n",
        "\n",
        "if TRAIN_MODEL:\n",
        "  history = model.fit_generator(reuters_gen.generate_pairs('train', bs=BATCH_SIZE),\n",
        "      #validation_data=reuters_gen.generate_pairs('test', bs=BATCH_SIZE),\n",
        "      steps_per_epoch=n_train//BATCH_SIZE,\n",
        "      #validation_steps=n_test//BATCH_SIZE,\n",
        "      epochs=EPOCHS, shuffle=True, callbacks=[cp])\n",
        "else:\n",
        "  list_of_files = glob.glob(os.path.join(OUTPUTDIR, basename + '*.hdf5'))\n",
        "  list_of_files = sorted(list_of_files, key=os.path.getctime)\n",
        "  assert(len(list_of_files) > 0)\n",
        "  model = load_model(list_of_files[-1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "314/314 [==============================] - 428s 1s/step - loss: 1.0100 - acc: 0.8962\n",
            "Epoch 2/10\n",
            "314/314 [==============================] - 421s 1s/step - loss: 0.8990 - acc: 0.8990\n",
            "Epoch 3/10\n",
            "314/314 [==============================] - 420s 1s/step - loss: 0.8884 - acc: 0.8990\n",
            "Epoch 4/10\n",
            "314/314 [==============================] - 420s 1s/step - loss: 0.8751 - acc: 0.8991\n",
            "Epoch 5/10\n",
            "314/314 [==============================] - 419s 1s/step - loss: 0.8659 - acc: 0.8990\n",
            "Epoch 6/10\n",
            "314/314 [==============================] - 419s 1s/step - loss: 0.8563 - acc: 0.8991\n",
            "Epoch 7/10\n",
            "314/314 [==============================] - 419s 1s/step - loss: 0.8497 - acc: 0.8989\n",
            "Epoch 8/10\n",
            "314/314 [==============================] - 420s 1s/step - loss: 0.8417 - acc: 0.8990\n",
            "Epoch 9/10\n",
            "314/314 [==============================] - 420s 1s/step - loss: 0.8360 - acc: 0.8990\n",
            "Epoch 10/10\n",
            "314/314 [==============================] - 419s 1s/step - loss: 0.8304 - acc: 0.8990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsIoTBfwnnqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_train_hat = next(reuters_gen.generate_pairs('train'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_V9D4-TTsCiM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "d6177963-1da1-45e1-98f6-e0679883059f"
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(np.argmax(model.predict(X_train[:20], verbose=1), axis=2))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r20/20 [==============================] - 1s 42ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "koH_AU6hsCiT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "76444f30-ea92-4740-a40e-11a12c52154e"
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(X_train[:20])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lt to charge corp said it to charges of to the social security administration and agreed to pay 1 2 mln dlrs in and costs to the u s government',\n",
              " 'the company also reached agreements in principle for an 8 1 mln dlr settlement of class action law',\n",
              " \"about 2 9 mln dlrs of the class action settlement will be provided by ' s insurance carrier\",\n",
              " 'the settlement is contingent on court approval after notice to class members it said',\n",
              " 'the case settlement all charges including and statement except for to which',\n",
              " \"the settlement includes the lifting of the government ' s suspension the of the federal civil claims suit and all charges against the individuals\",\n",
              " 'of the 2 9 mln dlrs the insurance carrier will provide for the civil settlement 750 000 dlrs will go to settle a lawsuit',\n",
              " 'for the year ended december 31 reported a net loss of 38 5 mln dlrs',\n",
              " 'the year end results include an 8 0 mln dlrs provision for future legal and or settlement costs to cover the civil and announced today',\n",
              " 'also said it named as president and chief executive officer robert who resigned as chairman and chief executive officer as part of the settlement of the',\n",
              " 'formerly served as executive vice president and chief operating officer',\n",
              " 'the company also said that due to the sluggish it does not expect to be profitable in the first quarter but is optimistic about the outlook for the year',\n",
              " 'for the first quarter of 1986 the company reported net income of 875 000 dlrs on sales of 66 0 mln dlrs',\n",
              " 'was among five executives who were charged along with three former officers in a 1985 federal from a 115 mln contract awarded to in 1981 to build a computer network for the social security administration',\n",
              " 'the were accused of to government officials and the social security administration',\n",
              " 'and other were also charged with providing testimony and justice during a securities and exchange commission investigation',\n",
              " 'under the settlement announced today federal agreed to all charges against and three other under a one year agreement',\n",
              " 'the charges would then be dropped if the successfully complete the period',\n",
              " 'details of the requirements in the agreement were not immediately available',\n",
              " 'and lt to make acquisition and inc said it has agreed to acquire electric parts distributor b and w electric supply co for an undisclosed amount of cash']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "WcYZnmYiG1Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXd7v4MBG1WW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iter_labels(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                yield reuters.categories(fid)\n",
        "labels_train = np.array(list(iter_labels('train')))\n",
        "labels_test = np.array(list(iter_labels('test')))\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer().fit(labels_train)\n",
        "y_train = mlb.transform(labels_train)\n",
        "y_test = mlb.transform(labels_test)\n",
        "\n",
        "def iter_sents(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                yield \" \".join(sent)\n",
        "data_train = np.array(list(iter_sents('train')))\n",
        "data_test = np.array(list(iter_sents('test')))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train)\n",
        "X_test = tokenizer.texts_to_sequences(data_test)\n",
        "\n",
        "X_train = pad_sequences(X_train, MAX_SEQUENCE_LEN)\n",
        "X_test = pad_sequences(X_test, MAX_SEQUENCE_LEN)\n",
        "\n",
        "def data_generator(X_in, batch_size=32, shuffle=True, repeat=True):\n",
        "    index = np.arange(X_in.shape[0])\n",
        "    while True:\n",
        "        np.random.shuffle(index)\n",
        "        X = X_in[index]\n",
        "        n = X.shape[0]//batch_size\n",
        "        for chunk in np.split(X[:n*batch_size], n):\n",
        "            yield chunk, to_categorical(chunk, MAX_NUM_WORDS)\n",
        "        rest = X[n*batch_size:]\n",
        "        if rest.shape[0]:\n",
        "            yield rest, to_categorical(rest, MAX_NUM_WORDS)\n",
        "        if not repeat:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_UemOL34sCiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_enc = Model(input_layer, encoder)\n",
        "vecs = model_enc.predict(X_hat[:1000], verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wekWSy1nsCic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "vecs_reduced = TSNE().fit_transform(vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M74X2ZjasCig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "categories = [(cat, len(reuters.fileids(categories=cat))) for cat in reuters.categories()]\n",
        "topn = [cat for cat, _ in sorted(categories, key=lambda x: -x[1])[:10]]\n",
        "\n",
        "indexes = []\n",
        "for cat in topn:\n",
        "    index = []\n",
        "    for pos, cats in enumerate(labels_train[:1000]):\n",
        "        if cat in cats:\n",
        "            index.append(pos)\n",
        "    indexes.append((cat, index))\n",
        "\n",
        "for cat, index in indexes:\n",
        "    plt.scatter(vecs_reduced[index,0], vecs_reduced[index,1], label=cat)\n",
        "plt.legend(bbox_to_anchor=(1, 1.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0AsvOddQsCio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}