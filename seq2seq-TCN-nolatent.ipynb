{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq-TCN.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "W1_uV8WoFXQu",
        "colab_type": "code",
        "outputId": "9d1d77c8-2d94-412b-d157-954ffaf4d792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p /content/drive/My\\ Drive/nn_output"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xscbWz8wGH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "OUTPUTDIR='/content/drive/My Drive/nn_output'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jrCPRQxBFJqP",
        "colab_type": "code",
        "outputId": "b69b8bab-a486-4a4f-a8e1-fcbc5a0edeeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install keras-TCN\n",
        "\n",
        "from keras.layers import (Bidirectional, Dense, Embedding, Input, Lambda, InputLayer, Reshape\n",
        "                          , LSTM, RepeatVector, TimeDistributed)\n",
        "from keras.models import Model, Sequential, load_model\n",
        "from tcn import TCN\n",
        "from keras.utils import to_categorical\n",
        "from keras import optimizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "import numpy as np\n",
        "from nltk.corpus import reuters\n",
        "from itertools import chain\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os.path\n",
        "import glob\n",
        "\n",
        "USE_GLOVE = True\n",
        "MAX_SEQUENCE_LEN = 100\n",
        "MAX_NUM_WORDS = 10000"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras-TCN in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-TCN) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (2.8.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.0.6)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-TCN) (1.11.0)\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "k5lh2ZJRsChy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from itertools import zip_longest\n",
        "def grouper(iterable, n, fillvalue=None):\n",
        "    \"Collect data into fixed-length chunks or blocks\"\n",
        "    args = [iter(iterable)] * n\n",
        "    return zip_longest(fillvalue=fillvalue, *args)\n",
        "\n",
        "class ReutersGenerator():\n",
        "    def __init__(self, max_seq_length=250, num_words=5000):\n",
        "        self.tok = Tokenizer(num_words=num_words)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_words = num_words\n",
        "    \n",
        "    def _gen_sents(self, fids):\n",
        "        return (' '.join(sent) for fid in fids for sent in reuters.sents(fid))\n",
        "    \n",
        "    def fit(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        self.tok.fit_on_texts(self._gen_sents(fids))\n",
        "        return self\n",
        "\n",
        "    def count(self, fid_startswith='train'):\n",
        "        fids = (fid for fid in reuters.fileids() if fid.startswith(fid_startswith))\n",
        "        return sum(1 for _ in self._gen_sents(fids))\n",
        "    \n",
        "    def inverse_transform(self, X):\n",
        "        return self.tok.sequences_to_texts(X)\n",
        "    \n",
        "    def generate_pairs(self, fid_startswith='train', bs=32, \n",
        "                         max_seq_len=250, forever=True, shuffle=True):\n",
        "        fids_in = np.array([fid for fid in reuters.fileids() if fid.startswith(fid_startswith)])\n",
        "        index = np.arange(fids_in.shape[0])\n",
        "        while True:\n",
        "            np.random.shuffle(index)\n",
        "            fids = fids_in[index]\n",
        "            sents = self._gen_sents(fids)\n",
        "            for batch in grouper(sents, bs):\n",
        "                seqs = self.tok.texts_to_sequences_generator(text for text in batch if text)\n",
        "                X = pad_sequences(list(seqs), self.max_seq_length)\n",
        "                yield X, to_categorical(X, self.num_words)\n",
        "            if not forever:\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SSZgXz1tHKqk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "reuters_gen = ReutersGenerator(\n",
        "    num_words=MAX_NUM_WORDS, max_seq_length=MAX_SEQUENCE_LEN).fit()\n",
        "n_train = reuters_gen.count('train')\n",
        "#n_test = reuters_gen.count('test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6v0kPz67sPl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "48590161-ce9d-49a8-d0bf-bf0dcda2fd1e"
      },
      "cell_type": "code",
      "source": [
        "if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "  !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "  !unzip \"glove.6B.zip\"\n",
        "\n",
        "# get glove coeff matrix\n",
        "embeddings_index = {}\n",
        "with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# prepare pre-learned embedding matrix\n",
        "embdedding_dim = 100\n",
        "word_index = reuters_gen.tok.word_index\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TXHnYTNY7zsD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        },
        "outputId": "65ff2892-6536-4616-fa04-21f726b63955"
      },
      "cell_type": "code",
      "source": [
        "kernel_size = 3\n",
        "n_dilations = 8\n",
        "n_hidden = 256\n",
        "embedding_size = 100\n",
        "dropout=0.4\n",
        "\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LEN,))\n",
        "encoder = None\n",
        "if not USE_GLOVE:\n",
        "  encoder = Embedding(MAX_NUM_WORDS, embedding_size)(input_layer)\n",
        "else:\n",
        "  encoder = Embedding(num_words, embdedding_dim, \n",
        "                      input_length=MAX_SEQUENCE_LEN, \n",
        "                      embeddings_initializer=Constant(embedding_matrix),\n",
        "                      trainable=True)(input_layer)\n",
        "encoder = TCN(return_sequences=True,\n",
        "              kernel_size=kernel_size,\n",
        "              dilations=[2**n for n in range(n_dilations)],\n",
        "              nb_filters=n_hidden,\n",
        "              nb_stacks=1,\n",
        "              dropout_rate=dropout)(encoder)\n",
        "output_layer = TimeDistributed(Dense(MAX_NUM_WORDS, activation='softmax'))(encoder)\n",
        "model = Model(input_layer, output_layer)\n",
        "optimizer = optimizers.Adam(lr=0.002, clipnorm=0.4)\n",
        "model.compile(optimizer=optimizer, metrics=['accuracy'], loss='categorical_crossentropy')\n",
        "print(model.summary())"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            (None, 100)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_5 (Embedding)         (None, 100, 100)     1000100     input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tcn_initial_conv (Conv1D)       (None, 100, 256)     25856       embedding_5[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_1_tanh_s0 (Co (None, 100, 256)     196864      tcn_initial_conv[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_1_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_33 (Lambda)              (None, 100, 256)     0           activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_1_s0_0.40 (None, 100, 256)     0           lambda_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_33 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_1_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_37 (Add)                    (None, 100, 256)     0           tcn_initial_conv[0][0]           \n",
            "                                                                 conv1d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_2_tanh_s0 (Co (None, 100, 256)     196864      add_37[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_2_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_34 (Lambda)              (None, 100, 256)     0           activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_2_s0_0.40 (None, 100, 256)     0           lambda_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_34 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_2_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_38 (Add)                    (None, 100, 256)     0           add_37[0][0]                     \n",
            "                                                                 conv1d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_4_tanh_s0 (Co (None, 100, 256)     196864      add_38[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_4_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_35 (Lambda)              (None, 100, 256)     0           activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_4_s0_0.40 (None, 100, 256)     0           lambda_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_35 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_4_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_39 (Add)                    (None, 100, 256)     0           add_38[0][0]                     \n",
            "                                                                 conv1d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_8_tanh_s0 (Co (None, 100, 256)     196864      add_39[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_8_tanh_s0[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "lambda_36 (Lambda)              (None, 100, 256)     0           activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_8_s0_0.40 (None, 100, 256)     0           lambda_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_36 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_8_s0_0.4000\n",
            "__________________________________________________________________________________________________\n",
            "add_40 (Add)                    (None, 100, 256)     0           add_39[0][0]                     \n",
            "                                                                 conv1d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_16_tanh_s0 (C (None, 100, 256)     196864      add_40[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_16_tanh_s0[0][0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_37 (Lambda)              (None, 100, 256)     0           activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_16_s0_0.4 (None, 100, 256)     0           lambda_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_37 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_16_s0_0.400\n",
            "__________________________________________________________________________________________________\n",
            "add_41 (Add)                    (None, 100, 256)     0           add_40[0][0]                     \n",
            "                                                                 conv1d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_32_tanh_s0 (C (None, 100, 256)     196864      add_41[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_32_tanh_s0[0][0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_38 (Lambda)              (None, 100, 256)     0           activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_32_s0_0.4 (None, 100, 256)     0           lambda_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_38 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_32_s0_0.400\n",
            "__________________________________________________________________________________________________\n",
            "add_42 (Add)                    (None, 100, 256)     0           add_41[0][0]                     \n",
            "                                                                 conv1d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_64_tanh_s0 (C (None, 100, 256)     196864      add_42[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_64_tanh_s0[0][0\n",
            "__________________________________________________________________________________________________\n",
            "lambda_39 (Lambda)              (None, 100, 256)     0           activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_64_s0_0.4 (None, 100, 256)     0           lambda_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_39 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_64_s0_0.400\n",
            "__________________________________________________________________________________________________\n",
            "add_43 (Add)                    (None, 100, 256)     0           add_42[0][0]                     \n",
            "                                                                 conv1d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tcn_d_causal_conv_128_tanh_s0 ( (None, 100, 256)     196864      add_43[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 100, 256)     0           tcn_d_causal_conv_128_tanh_s0[0][\n",
            "__________________________________________________________________________________________________\n",
            "lambda_40 (Lambda)              (None, 100, 256)     0           activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tcn_spatial_dropout1d_128_s0_0. (None, 100, 256)     0           lambda_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_40 (Conv1D)              (None, 100, 256)     65792       tcn_spatial_dropout1d_128_s0_0.40\n",
            "__________________________________________________________________________________________________\n",
            "add_45 (Add)                    (None, 100, 256)     0           conv1d_33[0][0]                  \n",
            "                                                                 conv1d_34[0][0]                  \n",
            "                                                                 conv1d_35[0][0]                  \n",
            "                                                                 conv1d_36[0][0]                  \n",
            "                                                                 conv1d_37[0][0]                  \n",
            "                                                                 conv1d_38[0][0]                  \n",
            "                                                                 conv1d_39[0][0]                  \n",
            "                                                                 conv1d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 100, 256)     0           add_45[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_5 (TimeDistrib (None, 100, 10000)   2570000     activation_45[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 5,697,204\n",
            "Trainable params: 5,697,204\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1MR8JitksCiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basename = 'seq2seq-TCN-model-small-nolatent'\n",
        "outfname = os.path.join(\n",
        "    OUTPUTDIR,\n",
        "    basename + '-ep{epoch:02d}.hdf5')\n",
        "cp = ModelCheckpoint(\n",
        "    outfname,\n",
        "    save_best_only=False,\n",
        "    save_weights_only=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L7AgCeZ3E_8D",
        "colab_type": "code",
        "outputId": "3a84a068-04ac-4ecc-f239-7ed8995153f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "TRAIN_MODEL = True\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "if TRAIN_MODEL:\n",
        "  history = model.fit_generator(reuters_gen.generate_pairs('train', bs=BATCH_SIZE),\n",
        "      #validation_data=reuters_gen.generate_pairs('test', bs=BATCH_SIZE),\n",
        "      steps_per_epoch=n_train//BATCH_SIZE,\n",
        "      #validation_steps=n_test//BATCH_SIZE,\n",
        "      epochs=EPOCHS, shuffle=True, callbacks=[cp])\n",
        "else:\n",
        "  list_of_files = glob.glob(os.path.join(OUTPUTDIR, basename + '*.hdf5'))\n",
        "  list_of_files = sorted(list_of_files, key=os.path.getctime)\n",
        "  assert(len(list_of_files) > 0)\n",
        "  model = load_model(list_of_files[-1])\n",
        "  print('Loaded model from \\'%s\\'' % list_of_files[-1])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "1258/1258 [==============================] - 264s 210ms/step - loss: 0.3338 - acc: 0.9591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VsIoTBfwnnqT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test, X_test_hat = next(reuters_gen.generate_pairs('test'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_V9D4-TTsCiM",
        "colab_type": "code",
        "outputId": "0b2410e1-49e9-411a-bcdb-5b901a77ddeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(np.argmax(model.predict(X_test[:20], verbose=1), axis=2))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r20/20 [==============================] - 1s 74ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chemlawn bank shares inc lt 1st qtr net shr 70 cts vs 67 cts net 6 152 000 vs 6 qtrly 000 note net includes pretax securities sales gains of 5 900 000 dlrs vs 5 900 000 dlrs',\n",
              " 'council meeting adjourned until thursday a council meeting of the international natural rubber organization has been adjourned until thursday as tomorrow is a malaysian national regularly officials of the organisation said',\n",
              " 'the main issue at the talks which opened here yesterday has been the buffer stock and the manner in which the buffer stock manager should continue to sell rubber after the current international pact for the commodity expires on october 22 they said',\n",
              " 'the deadline for the start of the new pact is january 1989',\n",
              " 'buffer stock manager has been to continue selling rubber during the recommendation period',\n",
              " 'other issues discussed include clause of the new accord the officials said',\n",
              " \"only malaysia of the pact ' s 32 producing and consuming member countries had amended the new agreement by the beginning of this month\",\n",
              " 'the meeting is due to end on thursday',\n",
              " 'wells fargo and co lt 3rd qtr net shr profit 2 77 dlrs vs profit 1 35 dlrs net 155 0 mln vs 77 4 mln nine mths shr loss 1 43 dlrs vs profit 3 par dlrs net loss 60 4 mln vs profit 195 2 mln assets 45 15 billion vs 42 69 billion loans 36 33 billion vs 34 46 billion deposits 29 7 billion vs 23 3 billion',\n",
              " 'systems corp lt 1st qtr net sept 30 end primary shr five cts vs eight cts diluted shr five cts vs eight cts net 100 000 vs 176 000 revs 4 030 000 vs 3 649 000 primary avg shrs 2 212 281 vs 2 189 000 diluted avg shrs 2 212 281 vs 2 330 866 note 1986 results includes a tax credit of 90 000 dlrs',\n",
              " 'ec commission dept oils and fats tax a spokesman for the european community commission defended the controversial plan for a levy on oils and fats saying that consumers would have to help dampen the surplus problem by paying the proposed tax',\n",
              " \"head of division of the commission ' s general for agriculture told the oils and fats contact days the commission firmly believes that the aegean which would be by community producers in the oils and fats sector would excessive asking consumers to make an appropriate contribution to the serious problem within that sector by paying a levy\",\n",
              " 'the proposed tax is necessary because the level of budgetary costs resulting from yellow oil and oilseeds production has become unacceptable said',\n",
              " 'recent estimates put these costs at 4 0 billion european currency units and by 1990 they would rise by another 2 0 billion ecus he said',\n",
              " \"in 1990 the community ' s standstill agreements with spain and portugal end and the ec would then feel the full impact of its\",\n",
              " 'the commission has proposed several cost and production cutting measures which include the introduction of a maximum guaranteed quantity system he added',\n",
              " \"under the commission ' s system for stabilising consumer prices in the oils and fats sector a reference price of 700 ecus per tonne for refined soy oil would be introduced said\",\n",
              " 'consumer prices could be raised or lowered by a regulatory amount when market prices are below or above this level',\n",
              " \"he said the revenue generated by charging a regulatory amount would be used to finance the common agricultural policy ' s oils and fats iraq\",\n",
              " 'the commission believes that hostile to the proposed tax have for the most part been based on or an analysis of the proposal he said']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "koH_AU6hsCiT",
        "colab_type": "code",
        "outputId": "d077d601-162a-4d03-cc5b-4a8e4f33e9b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "cell_type": "code",
      "source": [
        "reuters_gen.inverse_transform(X_test[:20])"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['amoskeag bank shares inc lt 1st qtr net shr 70 cts vs 67 cts net 6 416 000 vs 6 057 000 note net includes pretax securities sales gains of 5 900 000 dlrs vs 5 900 000 dlrs',\n",
              " 'council meeting adjourned until thursday a council meeting of the international natural rubber organization has been adjourned until thursday as tomorrow is a malaysian national holiday officials of the organisation said',\n",
              " 'the main issue at the talks which opened here yesterday has been the buffer stock and the manner in which the buffer stock manager should continue to sell rubber after the current international pact for the commodity expires on october 22 they said',\n",
              " 'the deadline for the start of the new pact is january 1989',\n",
              " 'buffer stock manager has been to continue selling rubber during the interim period',\n",
              " 'other issues discussed include ratification of the new accord the officials said',\n",
              " \"only malaysia of the pact ' s 32 producing and consuming member countries had ratified the new agreement by the beginning of this month\",\n",
              " 'the meeting is due to end on thursday',\n",
              " 'wells fargo and co lt 3rd qtr net shr profit 2 77 dlrs vs profit 1 35 dlrs net 155 0 mln vs 77 4 mln nine mths shr loss 1 43 dlrs vs profit 3 66 dlrs net loss 60 4 mln vs profit 195 2 mln assets 45 15 billion vs 42 69 billion loans 36 33 billion vs 34 46 billion deposits 29 7 billion vs 23 3 billion',\n",
              " 'systems corp lt 1st qtr net sept 30 end primary shr five cts vs eight cts diluted shr five cts vs eight cts net 100 000 vs 176 000 revs 4 027 000 vs 3 649 000 primary avg shrs 2 212 281 vs 2 189 000 diluted avg shrs 2 212 281 vs 2 330 866 note 1986 results includes a tax credit of 90 000 dlrs',\n",
              " 'ec commission defends oils and fats tax a spokesman for the european community commission defended the controversial plan for a levy on oils and fats saying that consumers would have to help alleviate the surplus problem by paying the proposed tax',\n",
              " \"head of division of the commission ' s general for agriculture told the oils and fats contact days the commission firmly believes that the sacrifices which would be by community producers in the oils and fats sector would justify asking consumers to make an appropriate contribution to the serious problem within that sector by paying a levy\",\n",
              " 'the proposed tax is necessary because the level of budgetary costs resulting from olive oil and oilseeds production has become unacceptable said',\n",
              " 'recent estimates put these costs at 4 0 billion european currency units and by 1990 they would rise by another 2 0 billion ecus he said',\n",
              " \"in 1990 the community ' s standstill agreements with spain and portugal end and the ec would then feel the full impact of its\",\n",
              " 'the commission has proposed several cost and production cutting measures which include the introduction of a maximum guaranteed quantity system he added',\n",
              " \"under the commission ' s system for stabilising consumer prices in the oils and fats sector a reference price of 700 ecus per tonne for refined soy oil would be introduced said\",\n",
              " 'consumer prices could be raised or lowered by a regulatory amount when market prices are below or above this level',\n",
              " \"he said the revenue generated by charging a regulatory amount would be used to finance the common agricultural policy ' s oils and fats regime\",\n",
              " 'the commission believes that hostile to the proposed tax have for the most part been based on or an analysis of the proposal he said']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "metadata": {
        "id": "WcYZnmYiG1Tw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pXd7v4MBG1WW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def iter_labels(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                yield reuters.categories(fid)\n",
        "labels_train = np.array(list(iter_labels('train')))\n",
        "labels_test = np.array(list(iter_labels('test')))\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer().fit(labels_train)\n",
        "y_train = mlb.transform(labels_train)\n",
        "y_test = mlb.transform(labels_test)\n",
        "\n",
        "def iter_sents(selection='train'):\n",
        "    for fid in reuters.fileids():\n",
        "        if fid.startswith(selection):\n",
        "            for sent in reuters.sents(fid):\n",
        "                yield \" \".join(sent)\n",
        "data_train = np.array(list(iter_sents('train')))\n",
        "data_test = np.array(list(iter_sents('test')))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(data_train)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(data_train)\n",
        "X_test = tokenizer.texts_to_sequences(data_test)\n",
        "\n",
        "X_train = pad_sequences(X_train, MAX_SEQUENCE_LEN)\n",
        "X_test = pad_sequences(X_test, MAX_SEQUENCE_LEN)\n",
        "\n",
        "def data_generator(X_in, batch_size=32, shuffle=True, repeat=True):\n",
        "    index = np.arange(X_in.shape[0])\n",
        "    while True:\n",
        "        np.random.shuffle(index)\n",
        "        X = X_in[index]\n",
        "        n = X.shape[0]//batch_size\n",
        "        for chunk in np.split(X[:n*batch_size], n):\n",
        "            yield chunk, to_categorical(chunk, MAX_NUM_WORDS)\n",
        "        rest = X[n*batch_size:]\n",
        "        if rest.shape[0]:\n",
        "            yield rest, to_categorical(rest, MAX_NUM_WORDS)\n",
        "        if not repeat:\n",
        "            break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_UemOL34sCiY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_enc = Model(input_layer, encoder)\n",
        "vecs = model_enc.predict(X_hat[:1000], verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wekWSy1nsCic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "vecs_reduced = TSNE().fit_transform(vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M74X2ZjasCig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "categories = [(cat, len(reuters.fileids(categories=cat))) for cat in reuters.categories()]\n",
        "topn = [cat for cat, _ in sorted(categories, key=lambda x: -x[1])[:10]]\n",
        "\n",
        "indexes = []\n",
        "for cat in topn:\n",
        "    index = []\n",
        "    for pos, cats in enumerate(labels_train[:1000]):\n",
        "        if cat in cats:\n",
        "            index.append(pos)\n",
        "    indexes.append((cat, index))\n",
        "\n",
        "for cat, index in indexes:\n",
        "    plt.scatter(vecs_reduced[index,0], vecs_reduced[index,1], label=cat)\n",
        "plt.legend(bbox_to_anchor=(1, 1.01))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0AsvOddQsCio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}