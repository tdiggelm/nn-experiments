{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_sequential.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdiggelm/nn-experiments/blob/master/attention_sequential.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fsBLbvaiZpUN",
        "colab_type": "code",
        "outputId": "d3304365-115a-4d48-b868-517e263cfc03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import layers, models\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy import stats\n",
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import datasets\n",
        "from IPython.core.display import HTML\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install keras-tcn\n",
        "from tcn import TCN"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.11.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cvh8Kg5hFkB7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Todo\n",
        "\n",
        "* try out `Scaled Dot-Product Attention` of `arXiv:1706.03762` for self-attention\n",
        "  * Querys = random weights (Dense) -OR- embedded input, Keys = Values = Hidden states LSTM\n",
        "* Apply fc layers seperately on each sequence encoded input (use `TimeDistributed`)\n",
        "* Normalize between the layers"
      ]
    },
    {
      "metadata": {
        "id": "IPzjKy1S2t9b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e69bac4f-8696-4e50-f4c0-f7bb066d435a"
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 250\n",
        "#MAX_NUM_WORDS = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data()\n",
        "\n",
        "X_train = pad_sequences(X_train, MAX_SEQ_LEN)\n",
        "X_test = pad_sequences(X_test, MAX_SEQ_LEN)\n",
        "\n",
        "word_index = datasets.imdb.get_word_index()\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "index_word = {}\n",
        "for k,v in word_index.items():\n",
        "  index_word[v] = k"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 4s 3us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4R-zBg_u1JhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import initializers\n",
        "from keras import layers\n",
        "def get_glove_embedding(word_index, input_length=None, trainable=True):\n",
        "  if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "    !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    !unzip \"glove.6B.zip\"\n",
        "\n",
        "  # get glove coeff matrix\n",
        "  embeddings_index = {}\n",
        "  with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # prepare pre-learned embedding matrix\n",
        "  embdedding_dim = 100\n",
        "  #num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "  num_words = len(word_index) + 1\n",
        "  embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      #if i > MAX_NUM_WORDS:\n",
        "      #    continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return layers.Embedding(num_words, embdedding_dim, \n",
        "                   input_length=input_length, \n",
        "                   embeddings_initializer=initializers.Constant(embedding_matrix),\n",
        "                   trainable=trainable)\n",
        "\n",
        "def frobenius_regularizer(weight_matrix):\n",
        "  A = K.softmax(weight_matrix, axis=1)\n",
        "  AT = K.transpose(A)\n",
        "  M = K.dot(A, AT)\n",
        "  d = K.shape(M)[1]\n",
        "  return 1.0 * K.sum(K.abs(M-K.eye(d))**2)\n",
        "\n",
        "def build_model(\n",
        "    n_hidden=50,\n",
        "    da=350,\n",
        "    r=30,\n",
        "    dropout=0.5,\n",
        "    lr=0.001, \n",
        "    clipnorm=0.1,\n",
        "    n_dense=1024\n",
        "):\n",
        "  inputs = layers.Input(shape=(MAX_SEQ_LEN,))\n",
        "  embedding = get_glove_embedding(word_index)(inputs)\n",
        "  H = layers.Bidirectional(layers.CuDNNLSTM(n_hidden,\n",
        "                                            return_sequences=True,\n",
        "                                           ))(embedding)\n",
        "  #--- BEGIN ATTENTION (arXiv:1703.03130)\n",
        "  WS1 = layers.Dense(da, activation='tanh')(H)\n",
        "  WS1 = layers.Dropout(dropout)(WS1)\n",
        "  WS2 = layers.Dense(r, kernel_regularizer=frobenius_regularizer)(WS1)\n",
        "  WS2 = layers.Dropout(dropout)(WS2)\n",
        "  A = layers.Softmax(axis=1, name='attention_weights')(WS2)\n",
        "  M = layers.Dot(axes=1)([A, H])\n",
        "  #--- END ATTENTION\n",
        "\n",
        "  reduced = layers.Lambda(lambda x: K.mean(x, axis=1))(M)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(reduced)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(dense)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  output = layers.Dense(1, activation='sigmoid')(dense)\n",
        "  model = models.Model(inputs, output)\n",
        "  optimizer = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
        "  model.compile(optimizer=optimizer, loss=['binary_crossentropy'],\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vT_00LXYd4eO",
        "colab_type": "code",
        "outputId": "25da1300-ff0d-49d0-a4d5-7dfe88b56bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "cell_type": "code",
      "source": [
        "model = build_model(**{\n",
        "    'clipnorm': 0.25,\n",
        "    'da': 100,\n",
        "    'dropout': 0.2,\n",
        "    'lr': 0.001,\n",
        "    'n_dense': 512,\n",
        "    'n_hidden': 100,\n",
        "    'r': 60}\n",
        ")\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            (None, 250)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 250, 100)     8858800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_2 (Bidirectional) (None, 250, 200)     161600      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 250, 100)     20100       bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 250, 100)     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 250, 60)      6060        dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 250, 60)      0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Softmax)     (None, 250, 60)      0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 60, 200)      0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 200)          0           dot_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 512)          102912      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 512)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 512)          262656      dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 1)            513         dropout_4[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 9,412,641\n",
            "Trainable params: 9,412,641\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m3d2K6F_gAlc",
        "colab_type": "code",
        "outputId": "cfd42f6f-ea78-4b4e-8380-06328b9bc8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_test[:1000], y_test[:1000]),\n",
        "                    epochs=3, verbose=1, batch_size=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 1000 samples\n",
            "Epoch 1/3\n",
            "15632/25000 [=================>............] - ETA: 56s - loss: 99.2071 - acc: 0.8133"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8h82N5SBeWIv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test, verbose=1)\n",
        "print(classification_report(y_test, y_pred>0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TbmdnmB6f-U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ax = pd.DataFrame(history.history).plot(logy=True)\n",
        "ax.set_xlabel(\"epoch\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kqF3X9iu_xz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def highlight_sentence(index_word, seqs, atts, y_true, y_pred, r='mean'):\n",
        "  html = ''\n",
        "  for seq, attention, yt, yp in zip(seqs, atts, y_true, y_pred):\n",
        "    begin = (seq>0).argmax()\n",
        "    mean_att = None\n",
        "    if r == 'mean':\n",
        "      mean_att = attention.mean(axis=1)\n",
        "    else:\n",
        "      mean_att = attention[:, r]\n",
        "    renorm = mean_att[begin:]/mean_att[begin:].max()\n",
        "    para = ''\n",
        "    for idx, score in zip(seq[begin:], renorm):\n",
        "      word = ''\n",
        "      try:\n",
        "        word = index_word[idx]\n",
        "      except KeyError:\n",
        "        pass\n",
        "      para += '<span style=\"background-color: rgba(100%%,0%%,0%%,%d%%)\">%s</span> ' % ((score)*100, word)\n",
        "    titel = '<h3>label (true/prediction): %d/%d</h3>' % (yt, yt)\n",
        "    html += titel + '<p>' + para + '</p>'\n",
        "  return html\n",
        "\n",
        "model_att = models.Model(inputs, model.get_layer('attention_weights').output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qAOriBdUvuSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "start = 50\n",
        "n = 10\n",
        "X = X_test[start:start+n]\n",
        "y_true = y_test[start:start+n]\n",
        "y_pred = model.predict(X)>0.5\n",
        "\n",
        "HTML(highlight_sentence(index_word, X, model_att.predict(X), y_true, y_pred, r='mean'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMFH51jEmWVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}