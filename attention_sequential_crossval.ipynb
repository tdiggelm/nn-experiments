{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_sequential_crossval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdiggelm/nn-experiments/blob/master/attention_sequential_crossval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fsBLbvaiZpUN",
        "colab_type": "code",
        "outputId": "e58d4d2d-42fb-414d-f15a-3a7b5f9b261b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy import stats\n",
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import datasets\n",
        "from IPython.core.display import HTML\n",
        "from sklearn.metrics import classification_report\n",
        "#!pip install keras-tcn\n",
        "#from tcn import TCN\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "\n",
        "outputdir = \"/content/gdrive/My Drive/ml_output/crossval\"\n",
        "!mkdir -p \"/content/gdrive/My Drive/ml_output/crossval\"\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.11.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IPzjKy1S2t9b",
        "colab_type": "code",
        "outputId": "9da391ae-ac87-4d6d-fa7c-554319cbb040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 250\n",
        "#MAX_NUM_WORDS = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data()\n",
        "\n",
        "X_train = pad_sequences(X_train, MAX_SEQ_LEN)\n",
        "X_test = pad_sequences(X_test, MAX_SEQ_LEN)\n",
        "\n",
        "word_index = datasets.imdb.get_word_index()\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "index_word = {}\n",
        "for k,v in word_index.items():\n",
        "  index_word[v] = k"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4R-zBg_u1JhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import initializers\n",
        "from keras import layers\n",
        "def get_glove_embedding(word_index, input_length=None, trainable=True):\n",
        "  if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "    !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    !unzip \"glove.6B.zip\"\n",
        "\n",
        "  # get glove coeff matrix\n",
        "  embeddings_index = {}\n",
        "  with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # prepare pre-learned embedding matrix\n",
        "  embdedding_dim = 100\n",
        "  #num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "  num_words = len(word_index) + 1\n",
        "  embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      #if i > MAX_NUM_WORDS:\n",
        "      #    continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return layers.Embedding(num_words, embdedding_dim, \n",
        "                   input_length=input_length, \n",
        "                   embeddings_initializer=initializers.Constant(embedding_matrix),\n",
        "                   trainable=trainable)\n",
        "\n",
        "def frobenius_regularizer(weight_matrix):\n",
        "  A = K.softmax(weight_matrix, axis=1)\n",
        "  AT = K.transpose(A)\n",
        "  M = K.dot(A, AT)\n",
        "  d = K.shape(M)[1]\n",
        "  return 1.0 * K.sum(K.abs(M-K.eye(d))**2)\n",
        "\n",
        "def build_model(\n",
        "    n_hidden=50,\n",
        "    da=350,\n",
        "    r=30,\n",
        "    dropout=0.5,\n",
        "    lr=0.001, \n",
        "    clipnorm=0.1,\n",
        "    n_dense=1024\n",
        "):\n",
        "  inputs = layers.Input(shape=(MAX_SEQ_LEN,))\n",
        "  embedding = get_glove_embedding(word_index)(inputs)\n",
        "  H = layers.Bidirectional(layers.CuDNNLSTM(n_hidden,\n",
        "                                            return_sequences=True,\n",
        "                                           ))(embedding)\n",
        "  #--- BEGIN ATTENTION (arXiv:1703.03130)\n",
        "  WS1 = layers.Dense(da, activation='tanh')(H)\n",
        "  WS1 = layers.Dropout(dropout)(WS1)\n",
        "  WS2 = layers.Dense(r, kernel_regularizer=frobenius_regularizer)(WS1)\n",
        "  WS2 = layers.Dropout(dropout)(WS2)\n",
        "  A = layers.Softmax(axis=1, name='attention_matrix')(WS2)\n",
        "  M = layers.Dot(axes=1)([A, H])\n",
        "  #--- END ATTENTION\n",
        "\n",
        "  reduced = layers.Lambda(lambda x: K.mean(x, axis=1))(M)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(reduced)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(dense)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  output = layers.Dense(1, activation='sigmoid')(dense)\n",
        "  model = models.Model(inputs, output)\n",
        "  optimizer = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
        "  model.compile(optimizer=optimizer, loss=['binary_crossentropy'],\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7thpakuSOUL",
        "colab_type": "code",
        "outputId": "7090721d-e29e-4f19-86f9-d85951d44a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4590
        }
      },
      "cell_type": "code",
      "source": [
        "param_dist = {'n_hidden': [50, 100, 150],\n",
        "              'da': [100, 350, 500],\n",
        "              'r': [10, 30, 60],\n",
        "              'dropout': [0.1, 0.2, 0.5],\n",
        "              'lr': [0.001, 0.0025, 0.005, 0.01],\n",
        "              'clipnorm': [0.0, 0.1, 0.25, 0.5],\n",
        "              'n_dense': [256, 512, 1024, 2048],\n",
        "              'batch_size': [8, 16, 32],\n",
        "              'epochs': [3]\n",
        "             }\n",
        "\n",
        "clfr = KerasClassifier(build_fn=build_model, verbose=1)\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(clfr, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=3)\n",
        "random_search.fit(X_train, y_train)\n",
        "df = pd.DataFrame(random_search.cv_results_)\n",
        "df.to_csv(\n",
        "    os.path.join(outputdir, 'attention_sequential_%s.csv' \n",
        "                 % datetime.today().isoformat(timespec='seconds')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 61s 4ms/step - loss: 2892.3962 - acc: 0.8292\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 59s 4ms/step - loss: 2891.1753 - acc: 0.9450\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 59s 4ms/step - loss: 2891.0609 - acc: 0.9891\n",
            "8334/8334 [==============================] - 9s 1ms/step\n",
            "16666/16666 [==============================] - 17s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 61s 4ms/step - loss: 2892.3409 - acc: 0.8267\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 59s 4ms/step - loss: 2891.1664 - acc: 0.9489\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 59s 4ms/step - loss: 2891.0652 - acc: 0.9866\n",
            "8333/8333 [==============================] - 9s 1ms/step\n",
            "16667/16667 [==============================] - 17s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 62s 4ms/step - loss: 2892.4515 - acc: 0.8268\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 59s 4ms/step - loss: 2891.1647 - acc: 0.9497\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 59s 4ms/step - loss: 2891.0559 - acc: 0.9880\n",
            "8333/8333 [==============================] - 10s 1ms/step\n",
            "16667/16667 [==============================] - 18s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 225s 14ms/step - loss: 323.5001 - acc: 0.8401\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 202s 12ms/step - loss: 313.5285 - acc: 0.9576\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 199s 12ms/step - loss: 313.4353 - acc: 0.9860\n",
            "8334/8334 [==============================] - 40s 5ms/step\n",
            "16666/16666 [==============================] - 80s 5ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 234s 14ms/step - loss: 323.5699 - acc: 0.8431\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 214s 13ms/step - loss: 313.5319 - acc: 0.9600\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 205s 12ms/step - loss: 313.4430 - acc: 0.9846\n",
            "8333/8333 [==============================] - 41s 5ms/step\n",
            "16667/16667 [==============================] - 81s 5ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 241s 14ms/step - loss: 323.3620 - acc: 0.8419\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 216s 13ms/step - loss: 313.5185 - acc: 0.9632\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 212s 13ms/step - loss: 313.4274 - acc: 0.9878\n",
            "8333/8333 [==============================] - 41s 5ms/step\n",
            "16667/16667 [==============================] - 80s 5ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 122s 7ms/step - loss: 1496.9163 - acc: 0.7826\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 118s 7ms/step - loss: 1496.2691 - acc: 0.9303\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 118s 7ms/step - loss: 1496.1335 - acc: 0.9745\n",
            "8334/8334 [==============================] - 19s 2ms/step\n",
            "16666/16666 [==============================] - 36s 2ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 122s 7ms/step - loss: 1496.8594 - acc: 0.8105\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 118s 7ms/step - loss: 1496.2244 - acc: 0.9395\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 118s 7ms/step - loss: 1496.1000 - acc: 0.9809\n",
            "8333/8333 [==============================] - 21s 2ms/step\n",
            "16667/16667 [==============================] - 39s 2ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 123s 7ms/step - loss: 1496.8530 - acc: 0.7854\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 118s 7ms/step - loss: 1496.2478 - acc: 0.9358\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 118s 7ms/step - loss: 1496.1304 - acc: 0.9747\n",
            "8333/8333 [==============================] - 20s 2ms/step\n",
            "16667/16667 [==============================] - 36s 2ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 74s 4ms/step - loss: 551.9855 - acc: 0.8035\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 69s 4ms/step - loss: 544.9613 - acc: 0.9086\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 69s 4ms/step - loss: 530.8749 - acc: 0.9603\n",
            "8334/8334 [==============================] - 12s 1ms/step\n",
            "16666/16666 [==============================] - 20s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 75s 4ms/step - loss: 552.1449 - acc: 0.8131\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 69s 4ms/step - loss: 545.6762 - acc: 0.9108\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 69s 4ms/step - loss: 530.2208 - acc: 0.9621\n",
            "8333/8333 [==============================] - 12s 1ms/step\n",
            "16667/16667 [==============================] - 20s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 75s 5ms/step - loss: 552.0779 - acc: 0.8057\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 69s 4ms/step - loss: 544.9530 - acc: 0.9069\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 69s 4ms/step - loss: 528.3883 - acc: 0.9624\n",
            "8333/8333 [==============================] - 12s 1ms/step\n",
            "16667/16667 [==============================] - 20s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 55s 3ms/step - loss: 526.6622 - acc: 0.8361\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 48s 3ms/step - loss: 493.9561 - acc: 0.9537\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 48s 3ms/step - loss: 493.8146 - acc: 0.9900\n",
            "8334/8334 [==============================] - 11s 1ms/step\n",
            "16666/16666 [==============================] - 17s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 55s 3ms/step - loss: 526.7889 - acc: 0.8370\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 48s 3ms/step - loss: 493.9590 - acc: 0.9546\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 48s 3ms/step - loss: 493.8087 - acc: 0.9915\n",
            "8333/8333 [==============================] - 12s 1ms/step\n",
            "16667/16667 [==============================] - 19s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 55s 3ms/step - loss: 526.2614 - acc: 0.8362\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 48s 3ms/step - loss: 493.9313 - acc: 0.9603\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 48s 3ms/step - loss: 493.8102 - acc: 0.9917\n",
            "8333/8333 [==============================] - 13s 2ms/step\n",
            "16667/16667 [==============================] - 20s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 113s 7ms/step - loss: 1496.7695 - acc: 0.8291\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 93s 6ms/step - loss: 1496.1347 - acc: 0.9563\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 93s 6ms/step - loss: 1496.0395 - acc: 0.9891\n",
            "8334/8334 [==============================] - 23s 3ms/step\n",
            "16666/16666 [==============================] - 42s 3ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 114s 7ms/step - loss: 1496.7687 - acc: 0.8333\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 93s 6ms/step - loss: 1496.1417 - acc: 0.9537\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 93s 6ms/step - loss: 1496.0455 - acc: 0.9876\n",
            "8333/8333 [==============================] - 24s 3ms/step\n",
            "16667/16667 [==============================] - 42s 3ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 122s 7ms/step - loss: 1496.7997 - acc: 0.8283\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 95s 6ms/step - loss: 1496.1384 - acc: 0.9531\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 93s 6ms/step - loss: 1496.0452 - acc: 0.9873\n",
            "8333/8333 [==============================] - 21s 3ms/step\n",
            "16667/16667 [==============================] - 40s 2ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 76s 5ms/step - loss: 454.6909 - acc: 0.8225\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 67s 4ms/step - loss: 434.4698 - acc: 0.9351\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 67s 4ms/step - loss: 433.8367 - acc: 0.9863\n",
            "8334/8334 [==============================] - 17s 2ms/step\n",
            "16666/16666 [==============================] - 26s 2ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 76s 5ms/step - loss: 454.9410 - acc: 0.8172\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 67s 4ms/step - loss: 434.5267 - acc: 0.9337\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 67s 4ms/step - loss: 433.8480 - acc: 0.9838\n",
            "8333/8333 [==============================] - 15s 2ms/step\n",
            "16667/16667 [==============================] - 22s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 77s 5ms/step - loss: 453.7458 - acc: 0.8123\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 67s 4ms/step - loss: 434.3808 - acc: 0.9341\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 67s 4ms/step - loss: 433.8453 - acc: 0.9831\n",
            "8333/8333 [==============================] - 14s 2ms/step\n",
            "16667/16667 [==============================] - 21s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 197s 12ms/step - loss: 98.3517 - acc: 0.8295\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 174s 10ms/step - loss: 77.2054 - acc: 0.9386\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 168s 10ms/step - loss: 75.5430 - acc: 0.9808\n",
            "8334/8334 [==============================] - 42s 5ms/step\n",
            "16666/16666 [==============================] - 75s 4ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 202s 12ms/step - loss: 99.4443 - acc: 0.8324\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 170s 10ms/step - loss: 78.3677 - acc: 0.9405\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 168s 10ms/step - loss: 75.5638 - acc: 0.9818\n",
            "8333/8333 [==============================] - 41s 5ms/step\n",
            "16667/16667 [==============================] - 76s 5ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 204s 12ms/step - loss: 98.5051 - acc: 0.8286\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 168s 10ms/step - loss: 77.2320 - acc: 0.9411\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 167s 10ms/step - loss: 75.5231 - acc: 0.9850\n",
            "8333/8333 [==============================] - 42s 5ms/step\n",
            "16667/16667 [==============================] - 76s 5ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 205s 12ms/step - loss: 318.2871 - acc: 0.8048\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 192s 12ms/step - loss: 313.6073 - acc: 0.9096\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 192s 12ms/step - loss: 313.5398 - acc: 0.9373\n",
            "8334/8334 [==============================] - 52s 6ms/step\n",
            "16666/16666 [==============================] - 96s 6ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 205s 12ms/step - loss: 318.1400 - acc: 0.7970\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 193s 12ms/step - loss: 313.6174 - acc: 0.9081\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 193s 12ms/step - loss: 313.5608 - acc: 0.9308\n",
            "8333/8333 [==============================] - 52s 6ms/step\n",
            "16667/16667 [==============================] - 95s 6ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 212s 13ms/step - loss: 318.2961 - acc: 0.8095\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 193s 12ms/step - loss: 313.6184 - acc: 0.9074\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 194s 12ms/step - loss: 313.5624 - acc: 0.9306\n",
            "8333/8333 [==============================] - 52s 6ms/step\n",
            "16667/16667 [==============================] - 95s 6ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 200s 12ms/step - loss: 96.9865 - acc: 0.8348\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 187s 11ms/step - loss: 65.3607 - acc: 0.9374\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 189s 11ms/step - loss: 41.7246 - acc: 0.9809\n",
            "8334/8334 [==============================] - 43s 5ms/step\n",
            "16666/16666 [==============================] - 70s 4ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 201s 12ms/step - loss: 97.2281 - acc: 0.8311\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 187s 11ms/step - loss: 65.9382 - acc: 0.9397\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 189s 11ms/step - loss: 41.1345 - acc: 0.9794\n",
            "8333/8333 [==============================] - 40s 5ms/step\n",
            "16667/16667 [==============================] - 68s 4ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16667/16667 [==============================] - 199s 12ms/step - loss: 96.8250 - acc: 0.8278\n",
            "Epoch 2/3\n",
            "16667/16667 [==============================] - 186s 11ms/step - loss: 62.2664 - acc: 0.9356\n",
            "Epoch 3/3\n",
            "16667/16667 [==============================] - 187s 11ms/step - loss: 41.5239 - acc: 0.9821\n",
            "1208/8333 [===>..........................] - ETA: 1:04"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zG_u2oVtL9um",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_fnames = glob(os.path.join(outputdir, '*.csv'))\n",
        "latest_report = max(all_fnames, key=os.path.getctime)\n",
        "df = pd.read_csv(latest_report, index_col=0) # load latest report\n",
        "df = df.sort_values(by=['mean_test_score'], ascending=False) # sort by best test score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YAvzcNF4QZzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "f7b7581c-d263-4ec2-c06b-30c7c6b4bb25"
      },
      "cell_type": "code",
      "source": [
        "df[['mean_test_score', 'mean_score_time', 'mean_fit_time'] + [k for k in df.keys() if k.startswith('param_')]]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>param_batch_size</th>\n",
              "      <th>param_clipnorm</th>\n",
              "      <th>param_da</th>\n",
              "      <th>param_dropout</th>\n",
              "      <th>param_epochs</th>\n",
              "      <th>param_lr</th>\n",
              "      <th>param_n_dense</th>\n",
              "      <th>param_n_hidden</th>\n",
              "      <th>param_r</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.88128</td>\n",
              "      <td>42.305274</td>\n",
              "      <td>589.129202</td>\n",
              "      <td>8</td>\n",
              "      <td>0.25</td>\n",
              "      <td>100</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>512</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.87676</td>\n",
              "      <td>11.983467</td>\n",
              "      <td>226.123119</td>\n",
              "      <td>32</td>\n",
              "      <td>0.50</td>\n",
              "      <td>500</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>2048</td>\n",
              "      <td>150</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.87492</td>\n",
              "      <td>41.675078</td>\n",
              "      <td>552.762389</td>\n",
              "      <td>8</td>\n",
              "      <td>0.50</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>1024</td>\n",
              "      <td>50</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.87268</td>\n",
              "      <td>15.471192</td>\n",
              "      <td>224.368373</td>\n",
              "      <td>32</td>\n",
              "      <td>0.25</td>\n",
              "      <td>350</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>2048</td>\n",
              "      <td>150</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.86780</td>\n",
              "      <td>11.668255</td>\n",
              "      <td>165.073447</td>\n",
              "      <td>32</td>\n",
              "      <td>0.25</td>\n",
              "      <td>500</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>512</td>\n",
              "      <td>50</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.86348</td>\n",
              "      <td>40.591257</td>\n",
              "      <td>663.278625</td>\n",
              "      <td>8</td>\n",
              "      <td>0.25</td>\n",
              "      <td>350</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>512</td>\n",
              "      <td>50</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.86064</td>\n",
              "      <td>9.380701</td>\n",
              "      <td>192.865873</td>\n",
              "      <td>32</td>\n",
              "      <td>0.10</td>\n",
              "      <td>500</td>\n",
              "      <td>0.2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>2048</td>\n",
              "      <td>100</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.86012</td>\n",
              "      <td>20.007912</td>\n",
              "      <td>372.759274</td>\n",
              "      <td>16</td>\n",
              "      <td>0.25</td>\n",
              "      <td>350</td>\n",
              "      <td>0.5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>512</td>\n",
              "      <td>150</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.85304</td>\n",
              "      <td>22.768712</td>\n",
              "      <td>316.548355</td>\n",
              "      <td>16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>350</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>2048</td>\n",
              "      <td>50</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.82848</td>\n",
              "      <td>52.054601</td>\n",
              "      <td>605.763170</td>\n",
              "      <td>8</td>\n",
              "      <td>0.00</td>\n",
              "      <td>350</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>256</td>\n",
              "      <td>100</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   mean_test_score  mean_score_time  mean_fit_time  param_batch_size  \\\n",
              "9          0.88128        42.305274     589.129202                 8   \n",
              "3          0.87676        11.983467     226.123119                32   \n",
              "7          0.87492        41.675078     552.762389                 8   \n",
              "6          0.87268        15.471192     224.368373                32   \n",
              "4          0.86780        11.668255     165.073447                32   \n",
              "1          0.86348        40.591257     663.278625                 8   \n",
              "0          0.86064         9.380701     192.865873                32   \n",
              "2          0.86012        20.007912     372.759274                16   \n",
              "5          0.85304        22.768712     316.548355                16   \n",
              "8          0.82848        52.054601     605.763170                 8   \n",
              "\n",
              "   param_clipnorm  param_da  param_dropout  param_epochs  param_lr  \\\n",
              "9            0.25       100            0.2             3    0.0010   \n",
              "3            0.50       500            0.1             3    0.0010   \n",
              "7            0.50       100            0.1             3    0.0010   \n",
              "6            0.25       350            0.5             3    0.0025   \n",
              "4            0.25       500            0.2             3    0.0050   \n",
              "1            0.25       350            0.1             3    0.0050   \n",
              "0            0.10       500            0.2             3    0.0050   \n",
              "2            0.25       350            0.5             3    0.0050   \n",
              "5            0.00       350            0.1             3    0.0050   \n",
              "8            0.00       350            0.1             3    0.0100   \n",
              "\n",
              "   param_n_dense  param_n_hidden  param_r  \n",
              "9            512             100       60  \n",
              "3           2048             150       60  \n",
              "7           1024              50       30  \n",
              "6           2048             150       30  \n",
              "4            512              50       60  \n",
              "1            512              50       60  \n",
              "0           2048             100       10  \n",
              "2            512             150       10  \n",
              "5           2048              50       10  \n",
              "8            256             100       60  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "m54T3B19M0YX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "be7d4ebd-a5c9-4e53-f7ad-0f8d8961ed38"
      },
      "cell_type": "code",
      "source": [
        "best_params = eval(df['params'].iloc[0])\n",
        "best_params"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 8,\n",
              " 'clipnorm': 0.25,\n",
              " 'da': 100,\n",
              " 'dropout': 0.2,\n",
              " 'epochs': 3,\n",
              " 'lr': 0.001,\n",
              " 'n_dense': 512,\n",
              " 'n_hidden': 100,\n",
              " 'r': 60}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "v-GXd48xQFyz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}