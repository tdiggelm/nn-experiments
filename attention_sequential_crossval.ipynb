{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_sequential_crossval.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tdiggelm/nn-experiments/blob/master/attention_sequential_crossval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fsBLbvaiZpUN",
        "colab_type": "code",
        "outputId": "09762a11-ef51-4e73-e16b-e85dc5db458f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from time import time\n",
        "from keras import layers, models\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from nltk.corpus import movie_reviews\n",
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from scipy import stats\n",
        "from keras import activations\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "from keras import datasets\n",
        "from IPython.core.display import HTML\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install keras-tcn\n",
        "from tcn import TCN\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from datetime import datetime"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: keras-tcn in /usr/local/lib/python3.6/dist-packages (2.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (1.14.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-tcn) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.0.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-tcn) (1.11.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IPzjKy1S2t9b",
        "colab_type": "code",
        "outputId": "9da391ae-ac87-4d6d-fa7c-554319cbb040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LEN = 250\n",
        "#MAX_NUM_WORDS = 10000\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = datasets.imdb.load_data()\n",
        "\n",
        "X_train = pad_sequences(X_train, MAX_SEQ_LEN)\n",
        "X_test = pad_sequences(X_test, MAX_SEQ_LEN)\n",
        "\n",
        "word_index = datasets.imdb.get_word_index()\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2\n",
        "\n",
        "index_word = {}\n",
        "for k,v in word_index.items():\n",
        "  index_word[v] = k"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 3s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4R-zBg_u1JhO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras import initializers\n",
        "from keras import layers\n",
        "def get_glove_embedding(word_index, input_length=None, trainable=True):\n",
        "  if not os.path.isfile(\"glove.6B.100d.txt\"):\n",
        "    !wget \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "    !unzip \"glove.6B.zip\"\n",
        "\n",
        "  # get glove coeff matrix\n",
        "  embeddings_index = {}\n",
        "  with open(\"glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
        "      for line in f:\n",
        "          values = line.split()\n",
        "          word = values[0]\n",
        "          coefs = np.asarray(values[1:], dtype='float32')\n",
        "          embeddings_index[word] = coefs\n",
        "  print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "  # prepare pre-learned embedding matrix\n",
        "  embdedding_dim = 100\n",
        "  #num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "  num_words = len(word_index) + 1\n",
        "  embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
        "  for word, i in word_index.items():\n",
        "      #if i > MAX_NUM_WORDS:\n",
        "      #    continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "          embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  return layers.Embedding(num_words, embdedding_dim, \n",
        "                   input_length=input_length, \n",
        "                   embeddings_initializer=initializers.Constant(embedding_matrix),\n",
        "                   trainable=trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7XxsgFfeBH5S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def frobenius_regularizer(weight_matrix):\n",
        "  A = K.softmax(weight_matrix, axis=1)\n",
        "  AT = K.transpose(A)\n",
        "  M = K.dot(A, AT)\n",
        "  d = K.shape(M)[1]\n",
        "  return 1.0 * K.sum(K.abs(M-K.eye(d))**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vT_00LXYd4eO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_model(\n",
        "    n_hidden=50,\n",
        "    da=350,\n",
        "    r=30,\n",
        "    dropout=0.5,\n",
        "    lr=0.001, \n",
        "    clipnorm=0.1,\n",
        "    n_dense=1024\n",
        "):\n",
        "  inputs = layers.Input(shape=(MAX_SEQ_LEN,))\n",
        "  embedding = get_glove_embedding(word_index)(inputs)\n",
        "  H = layers.Bidirectional(layers.CuDNNLSTM(n_hidden,\n",
        "                                            return_sequences=True,\n",
        "                                           ))(embedding)\n",
        "  #--- BEGIN ATTENTION (arXiv:1703.03130)\n",
        "  WS1 = layers.Dense(da, activation='tanh')(H)\n",
        "  WS1 = layers.Dropout(dropout)(WS1)\n",
        "  WS2 = layers.Dense(r, kernel_regularizer=frobenius_regularizer)(WS1)\n",
        "  WS2 = layers.Dropout(dropout)(WS2)\n",
        "  A = layers.Softmax(axis=1, name='attention_matrix')(WS2)\n",
        "  M = layers.Dot(axes=1)([A, H])\n",
        "  #--- END ATTENTION\n",
        "\n",
        "  reduced = layers.Lambda(lambda x: K.mean(x, axis=1))(M)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(reduced)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  dense = layers.Dense(n_dense, activation='relu')(dense)\n",
        "  dense = layers.Dropout(dropout)(dense)\n",
        "  output = layers.Dense(1, activation='sigmoid')(dense)\n",
        "  model = models.Model(inputs, output)\n",
        "  optimizer = optimizers.Adam(lr=lr, clipnorm=clipnorm)\n",
        "  model.compile(optimizer=optimizer, loss=['binary_crossentropy'],\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vnVdDVCv64cT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "outputdir = \"/content/gdrive/My Drive/ml_output/crossval\"\n",
        "!mkdir -p \"/content/gdrive/My Drive/ml_output/crossval\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z7thpakuSOUL",
        "colab_type": "code",
        "outputId": "7090721d-e29e-4f19-86f9-d85951d44a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "param_dist = {'n_hidden': [50, 100, 150],\n",
        "              'da': [100, 350, 500],\n",
        "              'r': [10, 30, 60],\n",
        "              'dropout': [0.1, 0.2, 0.5],\n",
        "              'lr': [0.001, 0.0025, 0.005, 0.01],\n",
        "              'clipnorm': [0.0, 0.1, 0.25, 0.5],\n",
        "              'n_dense': [256, 512, 1024, 2048],\n",
        "              'batch_size': [8, 16, 32],\n",
        "              'epochs': [3]\n",
        "             }\n",
        "\n",
        "clfr = KerasClassifier(build_fn=build_model, verbose=1)\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 10\n",
        "random_search = RandomizedSearchCV(clfr, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=3)\n",
        "random_search.fit(X_train, y_train)\n",
        "df = pd.DataFrame(random_search.cv_results_)\n",
        "df.to_csv(\n",
        "    os.path.join(outputdir, 'attention_sequential_%s.csv' \n",
        "                 % datetime.today().isoformat(timespec='seconds')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            "16666/16666 [==============================] - 61s 4ms/step - loss: 2892.3962 - acc: 0.8292\n",
            "Epoch 2/3\n",
            "16666/16666 [==============================] - 59s 4ms/step - loss: 2891.1753 - acc: 0.9450\n",
            "Epoch 3/3\n",
            "16666/16666 [==============================] - 59s 4ms/step - loss: 2891.0609 - acc: 0.9891\n",
            "8334/8334 [==============================] - 9s 1ms/step\n",
            "16666/16666 [==============================] - 17s 1ms/step\n",
            "Found 400000 word vectors.\n",
            "Epoch 1/3\n",
            " 3808/16667 [=====>........................] - ETA: 54s - loss: 2895.6485 - acc: 0.7188"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}