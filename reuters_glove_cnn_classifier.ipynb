{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Outlook for multiple classes: for multi-class prediction \n",
    "#     use softmax but for multi-label prediction use sigmoid\n",
    "# - Use aws deep learning instance: \n",
    "#     https://docs.aws.amazon.com/dlami/latest/devguide/keras-mxnet.html\n",
    "# - Use t-SNE to visualize the word embeddings\n",
    "# - Similarity search by using learned embeddings, i.e. \n",
    "#     https://blog.insightdatascience.com/the-unreasonable-effectiveness-of-deep-learning-representations-4ce83fc663cf\n",
    "#     (especially Spotify Annoy Index)\n",
    "# - Test out 1d-conv layers vs. stacked LSTM\n",
    "# - in order to handle arbitrary length input: \n",
    "#     (1) remove input_length from embedding layer,\n",
    "#     (2) batch wise apply pad_sequences to training input\n",
    "\n",
    "# TODO:\n",
    "# - multi-label, unbalanced dataset: use class_wheight, etc. (https://blog.mimacom.com/text-classification/, https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras)\n",
    "# - use CNN before or after LSTM layers\n",
    "# - use other metrics to assess model quality (recall, confusion matrix, etc.)\n",
    "# - implement http://www.aclweb.org/anthology/W18-0913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence \n",
    "from keras.layers import LSTM, Embedding, Dense, Flatten, Bidirectional, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import get_file\n",
    "from keras.initializers import Constant\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from nltk.corpus import reuters\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "MAX_NUM_WORDS = 5000\n",
    "MAX_SEQUENCE_LEN = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categs = sorted([(cat, len(reuters.fileids(categories=cat))) for cat in reuters.categories()], key=lambda x: -x[1])\n",
    "#categs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top_categories = [cat for cat, _ in categs[2:7]]\n",
    "#fileids = reuters.fileids(categories=top_categories)\n",
    "fileids = reuters.fileids()\n",
    "fileids_test = [fid for fid in fileids if fid.startswith(\"test\")]\n",
    "fileids_train = [fid for fid in fileids if fid.startswith(\"train\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(reuters.categories(fid) for fid in fileids_train)\n",
    "y_train = mlb.transform(reuters.categories(fid) for fid in fileids_train)\n",
    "y_test = mlb.transform(reuters.categories(fid) for fid in fileids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(reuters.raw(fid) for fid in fileids)\n",
    "X_train = tokenizer.texts_to_sequences(reuters.raw(fid) for fid in fileids_train)\n",
    "X_test = tokenizer.texts_to_sequences(reuters.raw(fid) for fid in fileids_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_SEQUENCE_LEN)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# get glove coeff matrix\n",
    "embeddings_index = {}\n",
    "fname = get_file(\"glove.6B.100d.txt\", \"http://nlp.stanford.edu/data/glove.6B.zip\", extract=True)\n",
    "with open(fname, encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# prepare pre-learned embedding matrix\n",
    "embdedding_dim = 100\n",
    "word_index = tokenizer.word_index\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, embdedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 500, 100)          500100    \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 500, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 496, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 95, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 19, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1024)              132096    \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 90)                92250     \n",
      "=================================================================\n",
      "Total params: 2,002,270\n",
      "Trainable params: 2,002,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "glove_embedding = Embedding(num_words, embdedding_dim, input_length=MAX_SEQUENCE_LEN,\n",
    "                            embeddings_initializer=Constant(embedding_matrix), trainable=True)\n",
    "\n",
    "# Build the model \n",
    "model = Sequential()\n",
    "model.add(glove_embedding)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dropout(0.1))\n",
    "#model.add(LSTM(100, return_sequences=True))\n",
    "#model.add(LSTM(100))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "#model.add(Dense(512, activation='relu', name='fc'))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Dense(mlb.classes_.shape[0], activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6992 samples, validate on 777 samples\n",
      "Epoch 1/10\n",
      "6992/6992 [==============================] - 115s 16ms/step - loss: 0.0546 - acc: 0.9863 - val_loss: 0.0332 - val_acc: 0.9924\n",
      "Epoch 2/10\n",
      "6992/6992 [==============================] - 133s 19ms/step - loss: 0.0291 - acc: 0.9921 - val_loss: 0.0239 - val_acc: 0.9932\n",
      "Epoch 3/10\n",
      "6992/6992 [==============================] - 111s 16ms/step - loss: 0.0228 - acc: 0.9932 - val_loss: 0.0201 - val_acc: 0.9943\n",
      "Epoch 4/10\n",
      "6992/6992 [==============================] - 102s 15ms/step - loss: 0.0191 - acc: 0.9942 - val_loss: 0.0167 - val_acc: 0.9949\n",
      "Epoch 5/10\n",
      "6992/6992 [==============================] - 103s 15ms/step - loss: 0.0165 - acc: 0.9948 - val_loss: 0.0146 - val_acc: 0.9953\n",
      "Epoch 6/10\n",
      "6992/6992 [==============================] - 108s 15ms/step - loss: 0.0141 - acc: 0.9955 - val_loss: 0.0141 - val_acc: 0.9957\n",
      "Epoch 7/10\n",
      "6992/6992 [==============================] - 122s 17ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.0133 - val_acc: 0.9959\n",
      "Epoch 8/10\n",
      "6992/6992 [==============================] - 122s 18ms/step - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0133 - val_acc: 0.9960\n",
      "Epoch 9/10\n",
      "6992/6992 [==============================] - 123s 18ms/step - loss: 0.0104 - acc: 0.9965 - val_loss: 0.0140 - val_acc: 0.9956\n",
      "Epoch 10/10\n",
      "6992/6992 [==============================] - 122s 17ms/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.0133 - val_acc: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a1c3f60>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3019/3019 [==============================] - 16s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "            acq       0.95      0.97      0.96       719\n",
      "           alum       0.00      0.00      0.00        23\n",
      "         barley       0.00      0.00      0.00        14\n",
      "            bop       0.45      0.43      0.44        30\n",
      "        carcass       0.58      0.39      0.47        18\n",
      "     castor-oil       0.00      0.00      0.00         1\n",
      "          cocoa       0.81      0.72      0.76        18\n",
      "        coconut       0.00      0.00      0.00         2\n",
      "    coconut-oil       0.00      0.00      0.00         3\n",
      "         coffee       0.96      0.96      0.96        28\n",
      "         copper       1.00      0.22      0.36        18\n",
      "     copra-cake       0.00      0.00      0.00         1\n",
      "           corn       0.87      0.73      0.80        56\n",
      "         cotton       0.00      0.00      0.00        20\n",
      "     cotton-oil       0.00      0.00      0.00         2\n",
      "            cpi       0.50      0.32      0.39        28\n",
      "            cpu       0.00      0.00      0.00         1\n",
      "          crude       0.85      0.90      0.87       189\n",
      "            dfl       0.00      0.00      0.00         1\n",
      "            dlr       0.47      0.91      0.62        44\n",
      "            dmk       0.00      0.00      0.00         4\n",
      "           earn       0.98      0.98      0.98      1087\n",
      "           fuel       0.00      0.00      0.00        10\n",
      "            gas       0.00      0.00      0.00        17\n",
      "            gnp       0.94      0.49      0.64        35\n",
      "           gold       0.60      0.93      0.73        30\n",
      "          grain       0.92      0.88      0.90       149\n",
      "      groundnut       0.00      0.00      0.00         4\n",
      "  groundnut-oil       0.00      0.00      0.00         1\n",
      "           heat       0.00      0.00      0.00         5\n",
      "            hog       0.00      0.00      0.00         6\n",
      "        housing       0.00      0.00      0.00         4\n",
      "         income       0.00      0.00      0.00         7\n",
      "    instal-debt       0.00      0.00      0.00         1\n",
      "       interest       0.87      0.65      0.74       131\n",
      "            ipi       0.92      0.92      0.92        12\n",
      "     iron-steel       0.00      0.00      0.00        14\n",
      "            jet       0.00      0.00      0.00         1\n",
      "           jobs       0.75      0.57      0.65        21\n",
      "       l-cattle       0.00      0.00      0.00         2\n",
      "           lead       0.00      0.00      0.00        14\n",
      "            lei       0.00      0.00      0.00         3\n",
      "        lin-oil       0.00      0.00      0.00         1\n",
      "      livestock       0.00      0.00      0.00        24\n",
      "         lumber       0.00      0.00      0.00         6\n",
      "      meal-feed       0.00      0.00      0.00        19\n",
      "       money-fx       0.70      0.96      0.81       179\n",
      "   money-supply       0.65      0.82      0.73        34\n",
      "        naphtha       0.00      0.00      0.00         4\n",
      "        nat-gas       0.37      0.57      0.45        30\n",
      "         nickel       0.00      0.00      0.00         1\n",
      "            nkr       0.00      0.00      0.00         2\n",
      "          nzdlr       0.00      0.00      0.00         2\n",
      "            oat       0.00      0.00      0.00         6\n",
      "        oilseed       0.67      0.09      0.15        47\n",
      "         orange       0.00      0.00      0.00        11\n",
      "      palladium       0.00      0.00      0.00         1\n",
      "       palm-oil       0.00      0.00      0.00        10\n",
      "     palmkernel       0.00      0.00      0.00         1\n",
      "       pet-chem       0.00      0.00      0.00        12\n",
      "       platinum       0.00      0.00      0.00         7\n",
      "         potato       0.00      0.00      0.00         3\n",
      "        propane       0.00      0.00      0.00         3\n",
      "           rand       0.00      0.00      0.00         1\n",
      "       rape-oil       0.00      0.00      0.00         3\n",
      "       rapeseed       0.00      0.00      0.00         9\n",
      "       reserves       0.79      0.61      0.69        18\n",
      "         retail       0.00      0.00      0.00         2\n",
      "           rice       0.00      0.00      0.00        24\n",
      "         rubber       1.00      0.42      0.59        12\n",
      "            rye       0.00      0.00      0.00         1\n",
      "           ship       0.91      0.69      0.78        89\n",
      "         silver       0.50      0.12      0.20         8\n",
      "        sorghum       0.00      0.00      0.00        10\n",
      "       soy-meal       0.00      0.00      0.00        13\n",
      "        soy-oil       0.00      0.00      0.00        11\n",
      "        soybean       0.75      0.09      0.16        33\n",
      "strategic-metal       0.00      0.00      0.00        11\n",
      "          sugar       1.00      0.86      0.93        36\n",
      "       sun-meal       0.00      0.00      0.00         1\n",
      "        sun-oil       0.00      0.00      0.00         2\n",
      "        sunseed       0.00      0.00      0.00         5\n",
      "            tea       0.00      0.00      0.00         4\n",
      "            tin       0.00      0.00      0.00        12\n",
      "          trade       0.79      0.85      0.81       117\n",
      "        veg-oil       0.94      0.46      0.62        37\n",
      "          wheat       0.90      0.73      0.81        71\n",
      "            wpi       1.00      0.20      0.33        10\n",
      "            yen       0.00      0.00      0.00        14\n",
      "           zinc       0.00      0.00      0.00        13\n",
      "\n",
      "      micro avg       0.88      0.77      0.82      3744\n",
      "      macro avg       0.27      0.22      0.22      3744\n",
      "   weighted avg       0.79      0.77      0.77      3744\n",
      "    samples avg       0.86      0.84      0.84      3744\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "PRED_THRESHOLD = 0.5\n",
    "print(classification_report(y_test, y_test_pred>PRED_THRESHOLD, target_names=mlb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "############################################# TESTING ###############################################################\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from skmultilearn.ext import Keras\n",
    "\n",
    "KERAS_PARAMS = dict(epochs=1, batch_size=32, verbose=1)\n",
    "\n",
    "def create_model_single_class(input_dim, output_dim):\n",
    "    # Build the model \n",
    "    model = Sequential()\n",
    "    model.add(glove_embedding)\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(32, return_sequences=True))\n",
    "    model.add(LSTM(32))\n",
    "    model.add(Dense(100, activation='relu', name='fc')) \n",
    "    model.add(Dense(output_dim, activation='sigmoid')) \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "clf = BinaryRelevance(classifier=Keras(create_model_single_class, False, KERAS_PARAMS), require_dense=[True,True])\n",
    "clf.fit(X_train, y_train)\n",
    "result = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.models import Model\n",
    "model_vec = Model(model.input, model.get_layer(name=\"fc\").output)\n",
    "vecs = model_vec.predict(X_test)\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "vecs_norm = normalize(vecs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "search_text = \"trade issues ec's with japan member states of the european community are starting to run out\"\n",
    "\n",
    "search_text_vec = model_vec.predict(sequence.pad_sequences(tokenizer.texts_to_sequences([search_text])))\n",
    "search_text_vec = normalize(search_text_vec)\n",
    "\n",
    "sorted_indices = np.argsort(-search_text_vec[0].dot(vecs_norm.T))\n",
    "print(mlb.inverse_transform(y_test[sorted_indices[:5]]))\n",
    "print(tokenizer.sequences_to_texts(X_test[sorted_indices[:5]]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorted_indices = np.argsort(-vecs_norm[0].dot(vecs_norm.T))\n",
    "print(mlb.inverse_transform(y_test[0:1]))\n",
    "print(tokenizer.sequences_to_texts(X_test[0:1]))\n",
    "\n",
    "print(mlb.inverse_transform(y_test[sorted_indices[:5]]))\n",
    "print(tokenizer.sequences_to_texts(X_test[sorted_indices[:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
